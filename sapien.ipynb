{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "sapien代码"
      ],
      "metadata": {
        "id": "NBvRtOrAtLun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpzP2MGMtJLk"
      },
      "outputs": [],
      "source": [
        "#demo3主程序\n",
        "import argparse\n",
        "from ast import parse\n",
        "from typing import Annotated\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import sapien.core as sapien\n",
        "from mani_skill.envs.sapien_env import BaseEnv\n",
        "import sapien.utils.viewer\n",
        "import h5py\n",
        "import json\n",
        "import mani_skill.trajectory.utils as trajectory_utils\n",
        "from mani_skill.utils import sapien_utils\n",
        "from mani_skill.utils.wrappers.record import RecordEpisode\n",
        "import tyro\n",
        "from dataclasses import dataclass\n",
        "import sapien\n",
        "from mani_skill.agents.base_agent import BaseAgent, Keyframe\n",
        "from mani_skill.agents.controllers import *\n",
        "from mani_skill.agents.registration import register_agent\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import mani_skill.envs\n",
        "import argparse\n",
        "from mani_skill.agents.controllers.pd_joint_vel import PDJointVelControllerConfig\n",
        "import mani_skill\n",
        "from mani_skill.agents.controllers.base_controller import DictController\n",
        "from copy import deepcopy\n",
        "from mani_skill.utils import gym_utils\n",
        "from mani_skill.utils.wrappers import RecordEpisode\n",
        "from typing import List, Optional, Annotated, Union\n",
        "import mplib\n",
        "import trimesh\n",
        "from mani_skill.envs.scene import ManiSkillScene\n",
        "from mani_skill.utils.structs.pose import to_sapien_pose\n",
        "import sapien.physx as physx\n",
        "from my_panda_motion_planner import MyPandaMotionPlanningSolver  # 新增\n",
        "\n",
        "\n",
        "@register_agent()\n",
        "class MyPanda(BaseAgent):\n",
        "    uid = \"my_panda\"\n",
        "    urdf_path = \"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.urdf\"\n",
        "    keyframes = dict(\n",
        "        standing=Keyframe(\n",
        "            pose=sapien.Pose(p=[1.4, -1.0, 0.01], q=R.from_euler(\"z\", 90, degrees=True).as_quat().tolist()),\n",
        "            qpos=np.array([\n",
        "                # arm: 7 joints\n",
        "                -1.75, -0.5, 0.0, 1.0, 0.0, -1.5, 1.4,\n",
        "                # -1.75, -0.35, -3.3, 1.4, 3.4, -1.1, 1.4,\n",
        "                # gripper: 6 joints\n",
        "                0.04,  # finger_joint\n",
        "                0.04, 0.04, 0.04, 0.04, 0.04  # mimic joints\n",
        "            ])\n",
        "        )\n",
        "    )\n",
        "    urdf_config = dict(\n",
        "        _materials=dict(\n",
        "            gripper=dict(static_friction=5.0, dynamic_friction=5.0, restitution=0.0)\n",
        "        ),\n",
        "        link=dict(\n",
        "            left_inner_finger_pad=dict(\n",
        "                material=\"gripper\", patch_radius=0.03, min_patch_radius=0.01\n",
        "            ),\n",
        "            right_inner_finger_pad=dict(\n",
        "                material=\"gripper\", patch_radius=0.03, min_patch_radius=0.01\n",
        "            ),\n",
        "            left_inner_finger=dict(material=\"gripper\"),\n",
        "            right_inner_finger=dict(material=\"gripper\"),\n",
        "        ),\n",
        "    )\n",
        "    def is_grasping(self, obj) -> bool:\n",
        "        # 在 robot.links 中查找 name 为 left_inner_finger_pad 和 right_inner_finger_pad 的 link\n",
        "        left_finger_pad = next(link for link in self.robot.links if link.get_name() == \"left_inner_finger_pad\")\n",
        "        right_finger_pad = next(link for link in self.robot.links if link.get_name() == \"right_inner_finger_pad\")\n",
        "\n",
        "        contacts = self.scene.get_contacts()\n",
        "\n",
        "        left_contact = any(\n",
        "            (c.actor0 == obj or c.actor1 == obj) and\n",
        "            (c.actor0 == left_finger_pad or c.actor1 == left_finger_pad)\n",
        "            for c in contacts\n",
        "        )\n",
        "        right_contact = any(\n",
        "            (c.actor0 == obj or c.actor1 == obj) and\n",
        "            (c.actor0 == right_finger_pad or c.actor1 == right_finger_pad)\n",
        "            for c in contacts\n",
        "        )\n",
        "\n",
        "        return left_contact and right_contact\n",
        "\n",
        "\n",
        "    @property\n",
        "    def _controller_configs(self):\n",
        "        arm_joint_names = [\n",
        "            \"joint_1\", \"joint_2\", \"joint_3\",\n",
        "            \"joint_4\", \"joint_5\", \"joint_6\", \"joint_7\",\n",
        "        ]\n",
        "        gripper_joint_names = [\"finger_joint\"]\n",
        "\n",
        "        arm_pd_joint_pos = PDJointPosControllerConfig(\n",
        "            arm_joint_names,\n",
        "            lower=[-3.14] * len(arm_joint_names),\n",
        "            upper=[3.14] * len(arm_joint_names),\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "            normalize_action=False,\n",
        "        )\n",
        "        arm_pd_joint_delta_pos = PDJointPosControllerConfig(\n",
        "            arm_joint_names,\n",
        "            lower=[-0.1] * len(arm_joint_names),\n",
        "            upper=[0.1] * len(arm_joint_names),\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "            use_delta=True,\n",
        "        )\n",
        "        gripper_pd_joint_pos = PDJointPosMimicControllerConfig(\n",
        "            gripper_joint_names,\n",
        "            lower=[0.00],\n",
        "            upper=[0.80],\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "        )\n",
        "\n",
        "        return deepcopy({\n",
        "            \"pd_joint_delta_pos\": {\n",
        "                \"arm\": arm_pd_joint_delta_pos,\n",
        "                \"gripper\": gripper_pd_joint_pos,\n",
        "            },\n",
        "            \"pd_joint_pos\": {\n",
        "                \"arm\": arm_pd_joint_pos,\n",
        "                \"gripper\": gripper_pd_joint_pos,\n",
        "            },\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    env_id: Annotated[str, tyro.conf.arg(aliases=[\"-e\"])] = \"Empty-v1\"\n",
        "    obs_mode: str = \"none\"\n",
        "    robot_uid: Annotated[str, tyro.conf.arg(aliases=[\"-r\"])] = \"my_panda\"\n",
        "    \"\"\"The robot to use. Robot setups supported for teleop in this script are panda and panda_stick\"\"\"\n",
        "    record_dir: str = \"demos\"\n",
        "    \"\"\"directory to record the demonstration data and optionally videos\"\"\"\n",
        "    save_video: bool = False\n",
        "    \"\"\"whether to save the videos of the demonstrations after collecting them all\"\"\"\n",
        "    viewer_shader: str = \"rt-fast\"\n",
        "    \"\"\"the shader to use for the viewer. 'default' is fast but lower-quality shader, 'rt' and 'rt-fast' are the ray tracing shaders\"\"\"\n",
        "    video_saving_shader: str = \"rt-fast\"\n",
        "    \"\"\"the shader to use for the videos of the demonstrations. 'minimal' is the fast shader, 'rt' and 'rt-fast' are the ray tracing shaders\"\"\"\n",
        "\n",
        "    keyframe: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-k\"])] = None\n",
        "    \"\"\"Name of keyframe to view\"\"\"\n",
        "    pause: Annotated[bool, tyro.conf.arg(aliases=[\"-p\"])] = False\n",
        "    \"\"\"Pause viewer on load\"\"\"\n",
        "\n",
        "\n",
        "def parse_args() -> Args:\n",
        "    return tyro.cli(Args)\n",
        "\n",
        "def main(args: Args):\n",
        "    output_dir = f\"{args.record_dir}/{args.env_id}/teleop/\"\n",
        "    env = gym.make(\n",
        "        args.env_id,\n",
        "        obs_mode=args.obs_mode,\n",
        "        control_mode=\"pd_joint_pos\",\n",
        "        render_mode=\"rgb_array\",\n",
        "        reward_mode=\"none\",\n",
        "        enable_shadow=True,\n",
        "        robot_uids=args.robot_uid,\n",
        "        viewer_camera_configs=dict(shader_pack=args.viewer_shader)\n",
        "    )\n",
        "    env = RecordEpisode(\n",
        "        env,\n",
        "        output_dir=output_dir,\n",
        "        trajectory_name=\"trajectory\",\n",
        "        save_video=False,\n",
        "        info_on_video=False,\n",
        "        source_type=\"teleoperation\",\n",
        "        source_desc=\"teleoperation via the click+drag system\"\n",
        "    )\n",
        "    num_trajs = 0\n",
        "    seed = 0\n",
        "    env.reset(\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # 初始化位姿\n",
        "    kf = None\n",
        "    if env.agent.keyframes:\n",
        "        kf_name = args.keyframe or next(iter(env.agent.keyframes))\n",
        "        kf = env.agent.keyframes[kf_name]\n",
        "        env.agent.robot.set_pose(kf.pose)\n",
        "        if kf.qpos is not None:\n",
        "            env.agent.robot.set_qpos(kf.qpos)\n",
        "        if kf.qvel is not None:\n",
        "            env.agent.robot.set_qvel(kf.qvel)\n",
        "        print(f\"📌 Viewing keyframe: {kf_name}\")\n",
        "\n",
        "    if env.gpu_sim_enabled:\n",
        "        env.scene._gpu_apply_all()\n",
        "        env.scene.px.gpu_update_articulation_kinematics()\n",
        "        env.scene._gpu_fetch_all()\n",
        "\n",
        "    viewer = env.render()\n",
        "    viewer.paused = args.pause\n",
        "\n",
        "    # ✅ 设置 robot base pose\n",
        "    from scipy.spatial.transform import Rotation as R\n",
        "    base_pose = sapien.Pose(\n",
        "        [1.4, -3.5, 0.01],  # 可按实际情况调整\n",
        "        R.from_euler(\"z\", 90, degrees=True).as_quat().tolist()\n",
        "    )\n",
        "\n",
        "\n",
        "    while True:\n",
        "        print(f\"Collecting trajectory {num_trajs+1}, seed={seed}\")\n",
        "        code = solve(env, debug=False, vis=True)\n",
        "        if code == \"quit\":\n",
        "            num_trajs += 1\n",
        "            break\n",
        "        elif code == \"continue\":\n",
        "            seed += 1\n",
        "            num_trajs += 1\n",
        "            env.reset(\n",
        "                seed=seed,\n",
        "            )\n",
        "            continue\n",
        "        elif code == \"restart\":\n",
        "            env.reset(\n",
        "                seed=seed,\n",
        "                options=dict(\n",
        "                    save_trajectory=False,\n",
        "                )\n",
        "            )\n",
        "    h5_file_path = env._h5_file.filename\n",
        "    json_file_path = env._json_path\n",
        "    env.close()\n",
        "    del env\n",
        "    print(f\"Trajectories saved to {h5_file_path}\")\n",
        "    if args.save_video:\n",
        "        print(f\"Saving videos to {output_dir}\")\n",
        "\n",
        "        trajectory_data = h5py.File(h5_file_path)\n",
        "        with open(json_file_path, \"r\") as f:\n",
        "            json_data = json.load(f)\n",
        "        env = gym.make(\n",
        "            args.env_id,\n",
        "            obs_mode=args.obs_mode,\n",
        "            control_mode=\"pd_joint_pos\",\n",
        "            render_mode=\"rgb_array\",\n",
        "            reward_mode=\"none\",\n",
        "            robot_uids=args.robot_uid,\n",
        "            human_render_camera_configs=dict(shader_pack=args.video_saving_shader),\n",
        "        )\n",
        "        env = RecordEpisode(\n",
        "            env,\n",
        "            output_dir=output_dir,\n",
        "            trajectory_name=\"trajectory\",\n",
        "            save_video=True,\n",
        "            info_on_video=False,\n",
        "            save_trajectory=False,\n",
        "            video_fps=30\n",
        "        )\n",
        "        for episode in json_data[\"episodes\"]:\n",
        "            traj_id = f\"traj_{episode['episode_id']}\"\n",
        "            data = trajectory_data[traj_id]\n",
        "            env.reset(**episode[\"reset_kwargs\"])\n",
        "            env_states_list = trajectory_utils.dict_to_list_of_dicts(data[\"env_states\"])\n",
        "\n",
        "            env.base_env.set_state_dict(env_states_list[0])\n",
        "            for action in np.array(data[\"actions\"]):\n",
        "                env.step(action)\n",
        "\n",
        "        trajectory_data.close()\n",
        "        env.close()\n",
        "        del env\n",
        "\n",
        "def solve(env: BaseEnv, debug=False, vis=False):\n",
        "    assert env.unwrapped.control_mode in [\n",
        "        \"pd_joint_pos\",\n",
        "        \"pd_joint_pos_vel\",\n",
        "    ], env.unwrapped.control_mode\n",
        "    robot_has_gripper = False\n",
        "\n",
        "    if env.unwrapped.robot_uids == \"my_panda\":  # 新增支持 MyPanda\n",
        "        robot_has_gripper = True\n",
        "        planner = MyPandaMotionPlanningSolver(\n",
        "            env,\n",
        "            debug=debug,\n",
        "            vis=vis,\n",
        "            base_pose=env.unwrapped.agent.robot.pose,\n",
        "            visualize_target_grasp_pose=False,\n",
        "            print_env_info=False,\n",
        "            joint_acc_limits=0.5,\n",
        "            joint_vel_limits=0.5,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported robot: {env.unwrapped.robot_uids}\")\n",
        "\n",
        "    viewer = env.render_human()\n",
        "\n",
        "    last_checkpoint_state = None\n",
        "    gripper_open = True\n",
        "    def select_panda_hand():\n",
        "        viewer.select_entity(sapien_utils.get_obj_by_name(env.agent.robot.links, \"tool_frame\")._objs[0].entity)  # 修改为 tool_frame\n",
        "    select_panda_hand()\n",
        "    for plugin in viewer.plugins:\n",
        "        if isinstance(plugin, sapien.utils.viewer.viewer.TransformWindow):\n",
        "            transform_window = plugin\n",
        "    while True:\n",
        "        transform_window.enabled = True\n",
        "        env.render_human()\n",
        "        execute_current_pose = False\n",
        "        if viewer.window.key_press(\"h\"):\n",
        "            print(\"\"\"Available commands:\n",
        "            h: print this help menu\n",
        "            g: toggle gripper to close/open (if there is a gripper)\n",
        "            u: move the panda hand up\n",
        "            j: move the panda hand down\n",
        "            arrow_keys: move the panda hand in the direction of the arrow keys\n",
        "            n: execute command via motion planning to make the robot move to the target pose indicated by the ghost panda arm\n",
        "            c: stop this episode and record the trajectory and move on to a new episode\n",
        "            q: quit the script and stop collecting data. Save trajectories and optionally videos.\n",
        "            \"\"\")\n",
        "        elif viewer.window.key_press(\"q\"):\n",
        "            return \"quit\"\n",
        "        elif viewer.window.key_press(\"c\"):\n",
        "            return \"continue\"\n",
        "        elif viewer.window.key_press(\"n\"):\n",
        "            execute_current_pose = True\n",
        "        elif viewer.window.key_press(\"g\") and robot_has_gripper:\n",
        "            print(\"gripper control\")\n",
        "            if gripper_open:\n",
        "                print(\"gripper open - close\")\n",
        "                gripper_open = False\n",
        "                _, reward, _, _, info = planner.close_gripper()\n",
        "            else:\n",
        "                print(\"gripper close - open\")\n",
        "                gripper_open = True\n",
        "                _, reward, _, _, info = planner.open_gripper()\n",
        "            print(f\"Reward: {reward}, Info: {info}\")\n",
        "        elif viewer.window.key_press(\"u\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[0, 0, -0.01])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        elif viewer.window.key_press(\"j\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[0, 0, +0.01])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        elif viewer.window.key_press(\"down\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[+0.01, 0, 0])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        elif viewer.window.key_press(\"up\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[-0.01, 0, 0])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        elif viewer.window.key_press(\"right\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[0, -0.01, 0])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        elif viewer.window.key_press(\"left\"):\n",
        "            select_panda_hand()\n",
        "            transform_window.gizmo_matrix = (transform_window._gizmo_pose * sapien.Pose(p=[0, +0.01, 0])).to_transformation_matrix()\n",
        "            transform_window.update_ghost_objects()\n",
        "        if execute_current_pose:\n",
        "            # # 替换姿态：设置为水平向量（绕 x/y/z 轴的旋转角度）\n",
        "            # target_pose = transform_window._gizmo_pose * sapien.Pose([0, 0, 0.1])\n",
        "            # r = R.from_euler('xyz', [0, 0, 0])  # 或 [0, 0, np.pi/2] 具体看你想让末端平行哪条轴\n",
        "            # target_pose.q = r.as_quat()\n",
        "            # result = planner.move_to_pose_with_screw(target_pose, dry_run=True)\n",
        "            result = planner.move_to_pose_with_screw(transform_window._gizmo_pose * sapien.Pose([0, 0, 0.1]), dry_run=True)\n",
        "            if result != -1 and len(result[\"position\"]) < 150:\n",
        "                _, reward, _, _, info = planner.follow_path(result)\n",
        "                print(f\"Reward: {reward}, Info: {info}\")\n",
        "            else:\n",
        "                if result == -1:\n",
        "                    print(\"Plan failed\")\n",
        "                else:\n",
        "                    print(\"Generated motion plan was too long. Try a closer sub-goal\")\n",
        "            execute_current_pose = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(parse_args())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#请在demo3求解器目录下里面运行\n",
        "shiqintong@shiqintong-Jiaolong-Series-MRID6:~/.local/lib/python3.10/site-packages/mani_skill/examples$ python3 -m mani_skill.examples.demo03 -r my_panda  -e \"RoboCasaKitchen-v1\"\n",
        "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.3.0) or chardet (4.0.0) doesn't match a supported version!\n",
        "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
        "2025-04-10 05:49:26,196 - mani_skill  - WARNING - my_panda is not in the task's list of supported robots. Code may not run as intended\n",
        "WARNING:root:Robot my_panda doesn't have a defined front facing size, defaulting to 0.7m\n",
        "2025-04-10 05:49:29,117 - mani_skill  - WARNING - mani_skill is not installed with git.\n",
        "/home/shiqintong/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.agent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent` for environment variables or `env.get_wrapper_attr('agent')` that will search the reminding wrappers.\n",
        "  logger.warn(\n",
        "📌 Viewing keyframe: standing\n",
        "/home/shiqintong/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.gpu_sim_enabled to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.gpu_sim_enabled` for environment variables or `env.get_wrapper_attr('gpu_sim_enabled')` that will search the reminding wrappers.\n",
        "  logger.warn(\n",
        "Collecting trajectory 1, seed=0\n",
        "\n",
        "=== Initializing Motion Planning Solver ===\n",
        "- Robot UID: my_panda\n",
        "- Control mode: pd_joint_pos\n",
        "- Base pose: Pose(raw_pose=tensor([[ 1.4000, -1.0000,  0.0100,  0.0000,  0.0000,  0.7071,  0.7071]]))\n",
        "- Joint vel limits: 0.5, acc limits: 0.5\n",
        "\n",
        "Robot Initial State:\n",
        "- Qpos: [-1.75 -0.5   0.    1.    0.   -1.5   1.4   0.04  0.04  0.04  0.04  0.04\n",
        "  0.04]\n",
        "/home/shiqintong/.local/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.render_human to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.render_human` for environment variables or `env.get_wrapper_attr('render_human')` that will search the reminding wrappers.\n",
        "  logger.warn(\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0843096  -0.62143683  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[-0.24097177]\n",
        " [-0.29286608]\n",
        " [ 0.09391727]\n",
        " [ 0.        ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]]\n",
        "Start qpos = [-1.75       -0.5         0.          1.          0.         -1.5\n",
        "  1.39999998  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.75       -0.5         0.          1.          0.         -1.5\n",
        "  1.39999998  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([68], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.112874   -0.62143683  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 0.027555050696384954\n",
        "Initial omega = [[-0.04973805]\n",
        " [ 0.01351322]\n",
        " [ 0.0019452 ]\n",
        " [ 0.00651   ]\n",
        " [ 0.02389754]\n",
        " [-0.01207511]]\n",
        "Start qpos = [-1.40837812 -1.07247722 -0.76377302  1.93014789 -0.28605628 -2.02220678\n",
        "  1.87674761  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 0.0276\n",
        "Initial qpos: [-1.40837812 -1.07247722 -0.76377302  1.93014789 -0.28605628 -2.02220678\n",
        "  1.87674761  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([85], dtype=torch.int32)}\n",
        "gripper control\n",
        "gripper open - close\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([91], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.112874  -0.5889796  1.6021563], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[-0.00097217]\n",
        " [ 0.00136578]\n",
        " [ 0.02936145]\n",
        " [ 0.        ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]]\n",
        "Start qpos = [-1.35878599 -1.05818331 -0.82271039  1.84942532 -0.32118884 -2.01015973\n",
        "  1.904158    0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.35878599 -1.05818331 -0.82271039  1.84942532 -0.32118884 -2.01015973\n",
        "  1.904158    0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([112], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.112874   -0.54069954  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[-0.00078145]\n",
        " [ 0.00063628]\n",
        " [ 0.04844625]\n",
        " [ 0.        ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]]\n",
        "Start qpos = [-1.38849187 -1.10174119 -0.81277204  1.81061542 -0.32311127 -1.92746556\n",
        "  1.9350642   0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.38849187 -1.10174119 -0.81277204  1.81061542 -0.32311127 -1.92746556\n",
        "  1.9350642   0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([132], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382  -0.54069954  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[0.02198686]\n",
        " [0.00104147]\n",
        " [0.00284703]\n",
        " [0.        ]\n",
        " [0.        ]\n",
        " [0.        ]]\n",
        "Start qpos = [-1.42865145 -1.16317487 -0.80347687  1.74164319 -0.32803977 -1.79875302\n",
        "  1.98488069  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.42865145 -1.16317487 -0.80347687  1.74164319 -0.32803977 -1.79875302\n",
        "  1.98488069  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([144], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382 -0.4942905  1.6021563], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[0.0022903 ]\n",
        " [0.0004684 ]\n",
        " [0.04540406]\n",
        " [0.        ]\n",
        " [0.        ]\n",
        " [0.        ]]\n",
        "Start qpos = [-1.47202647 -1.16953361 -0.76648414  1.77170396 -0.30532959 -1.79104257\n",
        "  1.97067881  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.47202647 -1.16953361 -0.76648414  1.77170396 -0.30532959 -1.79104257\n",
        "  1.97067881  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([164], dtype=torch.int32)}\n",
        "gripper control\n",
        "gripper close - open\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([170], dtype=torch.int32)}\n",
        "gripper control\n",
        "gripper open - close\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([176], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382  -0.47289333  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[-0.00152687]\n",
        " [ 0.00123507]\n",
        " [ 0.0238825 ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]]\n",
        "Start qpos = [-1.5033555  -1.21955502 -0.76115298  1.69657969 -0.31161901 -1.66715991\n",
        "  2.01480889  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.5033555  -1.21955502 -0.76115298  1.69657969 -0.31161901 -1.66715991\n",
        "  2.01480889  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([190], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382  -0.44604614  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 1\n",
        "Initial omega = [[-0.00137763]\n",
        " [ 0.0006873 ]\n",
        " [ 0.02920512]\n",
        " [ 0.        ]\n",
        " [ 0.        ]\n",
        " [ 0.        ]]\n",
        "Start qpos = [-1.51044083 -1.23841608 -0.7632435   1.64623797 -0.31787008 -1.60324168\n",
        "  2.03910875  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 1.0000\n",
        "Initial qpos: [-1.51044083 -1.23841608 -0.7632435   1.64623797 -0.31787008 -1.60324168\n",
        "  2.03910875  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([209], dtype=torch.int32)}\n",
        "gripper control\n",
        "gripper close - open\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([215], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382  -0.53558093  1.6021563 ], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n",
        " [-0.8757  0.8757]]\n",
        "Joint limit size: 13\n",
        "Qpos size: 13\n",
        "Relative theta = 0.023603784188018704\n",
        "Initial omega = [[ 0.01108832]\n",
        " [ 0.0083743 ]\n",
        " [-0.1117632 ]\n",
        " [ 0.02202276]\n",
        " [ 0.00838851]\n",
        " [ 0.00133022]]\n",
        "Start qpos = [-1.49839818 -1.25491333 -0.7838366   1.58519459 -0.34893078 -1.52853358\n",
        "  2.08528352  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "Start planning. Initial theta = 0.0236\n",
        "Initial qpos: [-1.49839818 -1.25491333 -0.7838366   1.58519459 -0.34893078 -1.52853358\n",
        "  2.08528352  0.          0.          0.          0.          0.\n",
        "  0.        ]\n",
        "- First attempt status: Success\n",
        "Reward: tensor([0.]), Info: {'elapsed_steps': tensor([244], dtype=torch.int32)}\n",
        "\n",
        "=== Attempting Screw Motion Planning ===\n",
        "- Target pose: P=[ 1.0892382 -0.67662    1.6021563], Q=[-0.15279359 -0.00195442  0.47976848  0.86398643]\n",
        "\n",
        "First planning attempt...\n",
        "Joint limits: [[-3.14    3.14  ]\n",
        " [-2.41    2.41  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.66    2.66  ]\n",
        " [-3.14    3.14  ]\n",
        " [-2.23    2.23  ]\n",
        " [-3.14    3.14  ]\n",
        " [ 0.      0.8   ]\n",
        " [-0.8757  0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.8757]\n",
        " [ 0.      0.81  ]\n"
      ],
      "metadata": {
        "id": "7poJeP0_tl6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#demo3的末端求解器\n",
        "import mplib\n",
        "import numpy as np\n",
        "import sapien\n",
        "import trimesh\n",
        "\n",
        "from mani_skill.agents.base_agent import BaseAgent\n",
        "from mani_skill.envs.sapien_env import BaseEnv\n",
        "from mani_skill.envs.scene import ManiSkillScene\n",
        "from mani_skill.utils.structs.pose import to_sapien_pose\n",
        "import sapien.physx as physx\n",
        "OPEN = 1\n",
        "CLOSED = -1\n",
        "\n",
        "\n",
        "class MyPandaMotionPlanningSolver:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: BaseEnv,\n",
        "        debug: bool = False,\n",
        "        vis: bool = True,\n",
        "        base_pose: sapien.Pose = None,  # TODO mplib doesn't support robot base being anywhere but 0\n",
        "        visualize_target_grasp_pose: bool = True,\n",
        "        print_env_info: bool = True,\n",
        "        joint_vel_limits=0.9,\n",
        "        joint_acc_limits=0.9,\n",
        "    ):\n",
        "        print(\"\\n=== Initializing Motion Planning Solver ===\")\n",
        "        print(f\"- Robot UID: {env.unwrapped.robot_uids}\")\n",
        "        print(f\"- Control mode: {env.unwrapped.control_mode}\")\n",
        "        print(f\"- Base pose: {base_pose}\")\n",
        "        print(f\"- Joint vel limits: {joint_vel_limits}, acc limits: {joint_acc_limits}\")\n",
        "        self.env = env\n",
        "        self.base_env: BaseEnv = env.unwrapped\n",
        "        self.env_agent: BaseAgent = self.base_env.agent\n",
        "        self.robot = self.env_agent.robot\n",
        "        self.joint_vel_limits = joint_vel_limits\n",
        "        self.joint_acc_limits = joint_acc_limits\n",
        "\n",
        "        self.base_pose = to_sapien_pose(base_pose)\n",
        "\n",
        "        self.planner = self.setup_planner()\n",
        "        self.control_mode = self.base_env.control_mode\n",
        "\n",
        "        self.debug = debug\n",
        "        self.vis = vis\n",
        "        self.print_env_info = print_env_info\n",
        "        self.visualize_target_grasp_pose = visualize_target_grasp_pose\n",
        "        self.gripper_state = OPEN\n",
        "        self.grasp_pose_visual = None\n",
        "        if self.vis and self.visualize_target_grasp_pose:\n",
        "            if \"grasp_pose_visual\" not in self.base_env.scene.actors:\n",
        "                self.grasp_pose_visual = build_panda_gripper_grasp_pose_visual(\n",
        "                    self.base_env.scene\n",
        "                )\n",
        "            else:\n",
        "                self.grasp_pose_visual = self.base_env.scene.actors[\"grasp_pose_visual\"]\n",
        "            self.grasp_pose_visual.set_pose(self.base_env.agent.tcp.pose)\n",
        "        self.elapsed_steps = 0\n",
        "\n",
        "        self.use_point_cloud = False\n",
        "        self.collision_pts_changed = False\n",
        "        self.all_collision_pts = None\n",
        "            # 打印机器人初始状态\n",
        "        print(\"\\nRobot Initial State:\")\n",
        "        print(f\"- Qpos: {self.robot.get_qpos().cpu().numpy()[0]}\")\n",
        "\n",
        "\n",
        "    def render_wait(self):\n",
        "        if not self.vis or not self.debug:\n",
        "            return\n",
        "        print(\"Press [c] to continue\")\n",
        "        viewer = self.base_env.render_human()\n",
        "        while True:\n",
        "            if viewer.window.key_down(\"c\"):\n",
        "                break\n",
        "            self.base_env.render_human()\n",
        "\n",
        "    def setup_planner(self):\n",
        "        planner = mplib.Planner(\n",
        "            urdf=\"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.urdf\",\n",
        "            srdf=\"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.srdf\",\n",
        "            move_group=\"tool_frame\",\n",
        "            joint_vel_limits=np.ones(7) * self.joint_vel_limits,\n",
        "            joint_acc_limits=np.ones(7) * self.joint_acc_limits,\n",
        "        )\n",
        "\n",
        "        planner.set_base_pose(np.hstack([self.base_pose.p, self.base_pose.q]))\n",
        "        return planner\n",
        "\n",
        "    def follow_path(self, result, refine_steps: int = 0):\n",
        "        n_step = result[\"position\"].shape[0]\n",
        "        for i in range(n_step + refine_steps):\n",
        "            qpos = result[\"position\"][min(i, n_step - 1)]\n",
        "            if self.control_mode == \"pd_joint_pos_vel\":\n",
        "                qvel = result[\"velocity\"][min(i, n_step - 1)]\n",
        "                action = np.hstack([qpos, qvel, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def move_to_pose_with_RRTConnect(\n",
        "        self, pose: sapien.Pose, dry_run: bool = False, refine_steps: int = 0\n",
        "    ):\n",
        "        pose = to_sapien_pose(pose)\n",
        "        if self.grasp_pose_visual is not None:\n",
        "            self.grasp_pose_visual.set_pose(pose)\n",
        "        pose = sapien.Pose(p=pose.p, q=pose.q)\n",
        "        result = self.planner.plan_qpos_to_pose(\n",
        "            np.concatenate([pose.p, pose.q]),\n",
        "            self.robot.get_qpos().cpu().numpy()[0],\n",
        "            time_step=self.base_env.control_timestep,\n",
        "            use_point_cloud=self.use_point_cloud,\n",
        "            wrt_world=True,\n",
        "        )\n",
        "        if result[\"status\"] != \"Success\":\n",
        "            print(result[\"status\"])\n",
        "            self.render_wait()\n",
        "            return -1\n",
        "        self.render_wait()\n",
        "        if dry_run:\n",
        "            return result\n",
        "        return self.follow_path(result, refine_steps=refine_steps)\n",
        "\n",
        "    def move_to_pose_with_screw(\n",
        "        self, pose: sapien.Pose, dry_run: bool = False, refine_steps: int = 0\n",
        "    ):\n",
        "        print(\"\\n=== Attempting Screw Motion Planning ===\")\n",
        "        pose = to_sapien_pose(pose)\n",
        "        print(f\"- Target pose: P={pose.p}, Q={pose.q}\")\n",
        "        # try screw two times before giving up\n",
        "        if self.grasp_pose_visual is not None:\n",
        "            self.grasp_pose_visual.set_pose(pose)\n",
        "        pose = sapien.Pose(p=pose.p , q=pose.q)\n",
        "        print(\"\\nFirst planning attempt...\")\n",
        "        result = self.planner.plan_screw(\n",
        "            np.concatenate([pose.p, pose.q]),\n",
        "            self.robot.get_qpos().cpu().numpy()[0][:7],\n",
        "            time_step=self.base_env.control_timestep,\n",
        "            use_point_cloud=self.use_point_cloud,\n",
        "        )\n",
        "        print(f\"- First attempt status: {result['status']}\")\n",
        "        if result[\"status\"] != \"Success\":\n",
        "            print(\"\\nSecond planning attempt...\")\n",
        "            result = self.planner.plan_screw(\n",
        "                np.concatenate([pose.p, pose.q]),\n",
        "                self.robot.get_qpos().cpu().numpy()[0][:7],\n",
        "                time_step=self.base_env.control_timestep,\n",
        "                use_point_cloud=self.use_point_cloud,\n",
        "            )\n",
        "            if result[\"status\"] != \"Success\":\n",
        "                print(\"\\n!!! Planning Failed !!!\")\n",
        "                print(f\"- Error status: {result['status']}\")\n",
        "                print(result[\"status\"])\n",
        "                self.render_wait()\n",
        "                return -1\n",
        "        self.render_wait()\n",
        "        if dry_run:\n",
        "            return result\n",
        "        return self.follow_path(result, refine_steps=refine_steps)\n",
        "\n",
        "    def open_gripper(self):\n",
        "        self.gripper_state = OPEN\n",
        "        qpos = self.robot.get_qpos()[0, :7].cpu().numpy()\n",
        "        for i in range(6):\n",
        "            if self.control_mode == \"pd_joint_pos\":\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, qpos * 0, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action[None])\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def close_gripper(self, t=6, gripper_state = CLOSED):\n",
        "        self.gripper_state = gripper_state\n",
        "        qpos = self.robot.get_qpos()[0, :7].cpu().numpy()\n",
        "        for i in range(t):\n",
        "            if self.control_mode == \"pd_joint_pos\":\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, qpos * 0, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action[None])\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def add_box_collision(self, extents: np.ndarray, pose: sapien.Pose):\n",
        "        self.use_point_cloud = True\n",
        "        box = trimesh.creation.box(extents, transform=pose.to_transformation_matrix())\n",
        "        pts, _ = trimesh.sample.sample_surface(box, 256)\n",
        "        if self.all_collision_pts is None:\n",
        "            self.all_collision_pts = pts\n",
        "        else:\n",
        "            self.all_collision_pts = np.vstack([self.all_collision_pts, pts])\n",
        "        self.planner.update_point_cloud(self.all_collision_pts)\n",
        "\n",
        "    def add_collision_pts(self, pts: np.ndarray):\n",
        "        if self.all_collision_pts is None:\n",
        "            self.all_collision_pts = pts\n",
        "        else:\n",
        "            self.all_collision_pts = np.vstack([self.all_collision_pts, pts])\n",
        "        self.planner.update_point_cloud(self.all_collision_pts)\n",
        "\n",
        "    def clear_collisions(self):\n",
        "        self.all_collision_pts = None\n",
        "        self.use_point_cloud = False\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "from transforms3d import quaternions\n",
        "\n",
        "\n",
        "def build_panda_gripper_grasp_pose_visual(scene: ManiSkillScene):\n",
        "    builder = scene.create_actor_builder()\n",
        "    grasp_pose_visual_width = 0.01\n",
        "    grasp_width = 0.05\n",
        "\n",
        "    builder.add_sphere_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, 0.0]),\n",
        "        radius=grasp_pose_visual_width,\n",
        "        material=sapien.render.RenderMaterial(base_color=[0.3, 0.4, 0.8, 0.7])\n",
        "    )\n",
        "\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, -0.08]),\n",
        "        half_size=[grasp_pose_visual_width, grasp_pose_visual_width, 0.02],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 1, 0, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, -0.05]),\n",
        "        half_size=[grasp_pose_visual_width, grasp_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 1, 0, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(\n",
        "            p=[\n",
        "                0.03 - grasp_pose_visual_width * 3,\n",
        "                grasp_width + grasp_pose_visual_width,\n",
        "                0.03 - 0.05,\n",
        "            ],\n",
        "            q=quaternions.axangle2quat(np.array([0, 1, 0]), theta=np.pi / 2),\n",
        "        ),\n",
        "        half_size=[0.04, grasp_pose_visual_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 0, 1, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(\n",
        "            p=[\n",
        "                0.03 - grasp_pose_visual_width * 3,\n",
        "                -grasp_width - grasp_pose_visual_width,\n",
        "                0.03 - 0.05,\n",
        "            ],\n",
        "            q=quaternions.axangle2quat(np.array([0, 1, 0]), theta=np.pi / 2),\n",
        "        ),\n",
        "        half_size=[0.04, grasp_pose_visual_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[1, 0, 0, 0.7]),\n",
        "    )\n",
        "    grasp_pose_visual = builder.build_kinematic(name=\"grasp_pose_visual\")\n",
        "    return grasp_pose_visual"
      ],
      "metadata": {
        "id": "JbDm53Ottbqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 单关节控制：demo2 执行指令：任何情况下运行  python3 -m mani_skill.examples.demo02 -r my_panda --random_actions  -e \"RoboCasaKitchen-v1\"\n",
        "\n",
        "import sapien\n",
        "import numpy as np\n",
        "from mani_skill.agents.base_agent import BaseAgent, Keyframe\n",
        "from mani_skill.agents.controllers import *\n",
        "from mani_skill.agents.registration import register_agent\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import mani_skill.envs\n",
        "import argparse\n",
        "\n",
        "from mani_skill.agents.controllers.pd_joint_vel import PDJointVelControllerConfig\n",
        "import gymnasium as gym\n",
        "import mani_skill\n",
        "from mani_skill.agents.controllers.base_controller import DictController\n",
        "from mani_skill.envs.sapien_env import BaseEnv\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import sapien\n",
        "\n",
        "from mani_skill.envs.sapien_env import BaseEnv\n",
        "from mani_skill.utils import gym_utils\n",
        "from mani_skill.utils.wrappers import RecordEpisode\n",
        "\n",
        "\n",
        "import tyro\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Annotated, Union\n",
        "\n",
        "\n",
        "import mplib\n",
        "import numpy as np\n",
        "import sapien\n",
        "import trimesh\n",
        "\n",
        "import mplib\n",
        "import numpy as np\n",
        "import sapien\n",
        "import trimesh\n",
        "\n",
        "from mani_skill.agents.base_agent import BaseAgent\n",
        "from mani_skill.envs.sapien_env import BaseEnv\n",
        "from mani_skill.envs.scene import ManiSkillScene\n",
        "from mani_skill.utils.structs.pose import to_sapien_pose\n",
        "import sapien.physx as physx\n",
        "OPEN = 1\n",
        "CLOSED = -1\n",
        "\n",
        "\n",
        "class MyPandaMotionPlanningSolver:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: BaseEnv,\n",
        "        debug: bool = False,\n",
        "        vis: bool = True,\n",
        "        base_pose: sapien.Pose = None,  # TODO mplib doesn't support robot base being anywhere but 0\n",
        "        visualize_target_grasp_pose: bool = True,\n",
        "        print_env_info: bool = True,\n",
        "        joint_vel_limits=0.9,\n",
        "        joint_acc_limits=0.9,\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.base_env: BaseEnv = env.unwrapped\n",
        "        self.env_agent: BaseAgent = self.base_env.agent\n",
        "        self.robot = self.env_agent.robot\n",
        "        self.joint_vel_limits = joint_vel_limits\n",
        "        self.joint_acc_limits = joint_acc_limits\n",
        "\n",
        "        self.base_pose = to_sapien_pose(base_pose)\n",
        "\n",
        "        self.planner = self.setup_planner()\n",
        "        self.control_mode = self.base_env.control_mode\n",
        "\n",
        "        self.debug = debug\n",
        "        self.vis = vis\n",
        "        self.print_env_info = print_env_info\n",
        "        self.visualize_target_grasp_pose = visualize_target_grasp_pose\n",
        "        self.gripper_state = OPEN\n",
        "        self.grasp_pose_visual = None\n",
        "        if self.vis and self.visualize_target_grasp_pose:\n",
        "            if \"grasp_pose_visual\" not in self.base_env.scene.actors:\n",
        "                self.grasp_pose_visual = build_panda_gripper_grasp_pose_visual(\n",
        "                    self.base_env.scene\n",
        "                )\n",
        "            else:\n",
        "                self.grasp_pose_visual = self.base_env.scene.actors[\"grasp_pose_visual\"]\n",
        "            self.grasp_pose_visual.set_pose(self.base_env.agent.tcp.pose)\n",
        "        self.elapsed_steps = 0\n",
        "\n",
        "        self.use_point_cloud = False\n",
        "        self.collision_pts_changed = False\n",
        "        self.all_collision_pts = None\n",
        "\n",
        "    def render_wait(self):\n",
        "        if not self.vis or not self.debug:\n",
        "            return\n",
        "        print(\"Press [c] to continue\")\n",
        "        viewer = self.base_env.render_human()\n",
        "        while True:\n",
        "            if viewer.window.key_down(\"c\"):\n",
        "                break\n",
        "            self.base_env.render_human()\n",
        "\n",
        "    def setup_planner(self):\n",
        "        link_names = [link.get_name() for link in self.robot.get_links()]\n",
        "        joint_names = [joint.get_name() for joint in self.robot.get_active_joints()]\n",
        "        planner = mplib.Planner(\n",
        "            urdf=\"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.urdf\",\n",
        "            srdf=\"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.srdf\",\n",
        "            user_link_names=link_names,\n",
        "            user_joint_names=joint_names,\n",
        "            move_group=\"tool_frame\",\n",
        "            joint_vel_limits=np.ones(7) * self.joint_vel_limits,\n",
        "            joint_acc_limits=np.ones(7) * self.joint_acc_limits,\n",
        "        )\n",
        "        planner.set_base_pose(np.hstack([self.base_pose.p, self.base_pose.q]))\n",
        "        return planner\n",
        "\n",
        "    def follow_path(self, result, refine_steps: int = 0):\n",
        "        n_step = result[\"position\"].shape[0]\n",
        "        for i in range(n_step + refine_steps):\n",
        "            qpos = result[\"position\"][min(i, n_step - 1)]\n",
        "            if self.control_mode == \"pd_joint_pos_vel\":\n",
        "                qvel = result[\"velocity\"][min(i, n_step - 1)]\n",
        "                action = np.hstack([qpos, qvel, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def move_to_pose_with_RRTConnect(\n",
        "        self, pose: sapien.Pose, dry_run: bool = False, refine_steps: int = 0\n",
        "    ):\n",
        "        pose = to_sapien_pose(pose)\n",
        "        if self.grasp_pose_visual is not None:\n",
        "            self.grasp_pose_visual.set_pose(pose)\n",
        "        pose = sapien.Pose(p=pose.p, q=pose.q)\n",
        "        result = self.planner.plan_qpos_to_pose(\n",
        "            np.concatenate([pose.p, pose.q]),\n",
        "            self.robot.get_qpos().cpu().numpy()[0],\n",
        "            time_step=self.base_env.control_timestep,\n",
        "            use_point_cloud=self.use_point_cloud,\n",
        "            wrt_world=True,\n",
        "        )\n",
        "        if result[\"status\"] != \"Success\":\n",
        "            print(result[\"status\"])\n",
        "            self.render_wait()\n",
        "            return -1\n",
        "        self.render_wait()\n",
        "        if dry_run:\n",
        "            return result\n",
        "        return self.follow_path(result, refine_steps=refine_steps)\n",
        "\n",
        "    def move_to_pose_with_screw(\n",
        "        self, pose: sapien.Pose, dry_run: bool = False, refine_steps: int = 0\n",
        "    ):\n",
        "        pose = to_sapien_pose(pose)\n",
        "        # try screw two times before giving up\n",
        "        if self.grasp_pose_visual is not None:\n",
        "            self.grasp_pose_visual.set_pose(pose)\n",
        "        pose = sapien.Pose(p=pose.p , q=pose.q)\n",
        "        result = self.planner.plan_screw(\n",
        "            np.concatenate([pose.p, pose.q]),\n",
        "            self.robot.get_qpos().cpu().numpy()[0],\n",
        "            time_step=self.base_env.control_timestep,\n",
        "            use_point_cloud=self.use_point_cloud,\n",
        "        )\n",
        "        if result[\"status\"] != \"Success\":\n",
        "            result = self.planner.plan_screw(\n",
        "                np.concatenate([pose.p, pose.q]),\n",
        "                self.robot.get_qpos().cpu().numpy()[0],\n",
        "                time_step=self.base_env.control_timestep,\n",
        "                use_point_cloud=self.use_point_cloud,\n",
        "            )\n",
        "            if result[\"status\"] != \"Success\":\n",
        "                print(result[\"status\"])\n",
        "                self.render_wait()\n",
        "                return -1\n",
        "        self.render_wait()\n",
        "        if dry_run:\n",
        "            return result\n",
        "        return self.follow_path(result, refine_steps=refine_steps)\n",
        "\n",
        "    def open_gripper(self):\n",
        "        self.gripper_state = OPEN\n",
        "        qpos = self.robot.get_qpos()[0, :-2].cpu().numpy()\n",
        "        for i in range(6):\n",
        "            if self.control_mode == \"pd_joint_pos\":\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, qpos * 0, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def close_gripper(self, t=6, gripper_state = CLOSED):\n",
        "        self.gripper_state = gripper_state\n",
        "        qpos = self.robot.get_qpos()[0, :-2].cpu().numpy()\n",
        "        for i in range(t):\n",
        "            if self.control_mode == \"pd_joint_pos\":\n",
        "                action = np.hstack([qpos, self.gripper_state])\n",
        "            else:\n",
        "                action = np.hstack([qpos, qpos * 0, self.gripper_state])\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self.elapsed_steps += 1\n",
        "            if self.print_env_info:\n",
        "                print(\n",
        "                    f\"[{self.elapsed_steps:3}] Env Output: reward={reward} info={info}\"\n",
        "                )\n",
        "            if self.vis:\n",
        "                self.base_env.render_human()\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def add_box_collision(self, extents: np.ndarray, pose: sapien.Pose):\n",
        "        self.use_point_cloud = True\n",
        "        box = trimesh.creation.box(extents, transform=pose.to_transformation_matrix())\n",
        "        pts, _ = trimesh.sample.sample_surface(box, 256)\n",
        "        if self.all_collision_pts is None:\n",
        "            self.all_collision_pts = pts\n",
        "        else:\n",
        "            self.all_collision_pts = np.vstack([self.all_collision_pts, pts])\n",
        "        self.planner.update_point_cloud(self.all_collision_pts)\n",
        "\n",
        "    def add_collision_pts(self, pts: np.ndarray):\n",
        "        if self.all_collision_pts is None:\n",
        "            self.all_collision_pts = pts\n",
        "        else:\n",
        "            self.all_collision_pts = np.vstack([self.all_collision_pts, pts])\n",
        "        self.planner.update_point_cloud(self.all_collision_pts)\n",
        "\n",
        "    def clear_collisions(self):\n",
        "        self.all_collision_pts = None\n",
        "        self.use_point_cloud = False\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "from transforms3d import quaternions\n",
        "\n",
        "\n",
        "def build_panda_gripper_grasp_pose_visual(scene: ManiSkillScene):\n",
        "    builder = scene.create_actor_builder()\n",
        "    grasp_pose_visual_width = 0.01\n",
        "    grasp_width = 0.05\n",
        "\n",
        "    builder.add_sphere_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, 0.0]),\n",
        "        radius=grasp_pose_visual_width,\n",
        "        material=sapien.render.RenderMaterial(base_color=[0.3, 0.4, 0.8, 0.7])\n",
        "    )\n",
        "\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, -0.08]),\n",
        "        half_size=[grasp_pose_visual_width, grasp_pose_visual_width, 0.02],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 1, 0, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(p=[0, 0, -0.05]),\n",
        "        half_size=[grasp_pose_visual_width, grasp_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 1, 0, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(\n",
        "            p=[\n",
        "                0.03 - grasp_pose_visual_width * 3,\n",
        "                grasp_width + grasp_pose_visual_width,\n",
        "                0.03 - 0.05,\n",
        "            ],\n",
        "            q=quaternions.axangle2quat(np.array([0, 1, 0]), theta=np.pi / 2),\n",
        "        ),\n",
        "        half_size=[0.04, grasp_pose_visual_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[0, 0, 1, 0.7]),\n",
        "    )\n",
        "    builder.add_box_visual(\n",
        "        pose=sapien.Pose(\n",
        "            p=[\n",
        "                0.03 - grasp_pose_visual_width * 3,\n",
        "                -grasp_width - grasp_pose_visual_width,\n",
        "                0.03 - 0.05,\n",
        "            ],\n",
        "            q=quaternions.axangle2quat(np.array([0, 1, 0]), theta=np.pi / 2),\n",
        "        ),\n",
        "        half_size=[0.04, grasp_pose_visual_width, grasp_pose_visual_width],\n",
        "        material=sapien.render.RenderMaterial(base_color=[1, 0, 0, 0.7]),\n",
        "    )\n",
        "    grasp_pose_visual = builder.build_kinematic(name=\"grasp_pose_visual\")\n",
        "    return grasp_pose_visual\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_args(args=None):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-r\", \"--robot-uid\", type=str, default=\"panda\", help=\"The id of the robot to place in the environment\")\n",
        "    parser.add_argument(\"-b\", \"--sim-backend\", type=str, default=\"auto\", help=\"Which simulation backend to use. Can be 'auto', 'cpu', 'gpu'\")\n",
        "    parser.add_argument(\"-c\", \"--control-mode\", type=str, default=\"pd_joint_pos\", help=\"The control mode to use. Note that for new robots being implemented if the _controller_configs is not implemented in the selected robot, we by default provide two default controllers, 'pd_joint_pos' and 'pd_joint_delta_pos' \")\n",
        "    parser.add_argument(\"-k\", \"--keyframe\", type=str, help=\"The name of the keyframe of the robot to display\")\n",
        "    parser.add_argument(\"--shader\", default=\"default\", type=str, help=\"Change shader used for rendering. Default is 'default' which is very fast. Can also be 'rt' for ray tracing and generating photo-realistic renders. Can also be 'rt-fast' for a faster but lower quality ray-traced renderer\")\n",
        "    parser.add_argument(\"--keyframe-actions\", action=\"store_true\", help=\"Whether to use the selected keyframe to set joint targets to try and hold the robot in its position\")\n",
        "    parser.add_argument(\"--random-actions\", action=\"store_true\", help=\"Whether to sample random actions to control the agent. If False, no control signals are sent and it is just rendering.\")\n",
        "    parser.add_argument(\"--none-actions\", action=\"store_true\", help=\"If set, then the scene and rendering will update each timestep but no joints will be controlled via code. You can use this to control the robot freely via the GUI.\")\n",
        "    parser.add_argument(\"--zero-actions\", action=\"store_true\", help=\"Whether to send zero actions to the robot. If False, no control signals are sent and it is just rendering.\")\n",
        "    parser.add_argument(\"--sim-freq\", type=int, default=100, help=\"Simulation frequency\")\n",
        "    parser.add_argument(\"--control-freq\", type=int, default=20, help=\"Control frequency\")\n",
        "    parser.add_argument(\n",
        "        \"-s\",\n",
        "        \"--seed\",\n",
        "        type=int,\n",
        "        help=\"Seed the random actions and environment. Default is no seed\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    env_id: Annotated[str, tyro.conf.arg(aliases=[\"-e\"])] = \"Empty-v1\"\n",
        "    \"\"\"Environment ID (e.g. PushCube-v1, Empty-v1)\"\"\"\n",
        "\n",
        "    robot_uids: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-r\"])] = \"panda\"\n",
        "    \"\"\"Robot UID(s). Comma-separated or single string. Default: panda\"\"\"\n",
        "\n",
        "    sim_backend: Annotated[str, tyro.conf.arg(aliases=[\"-b\"])] = \"auto\"\n",
        "    \"\"\"Simulation backend: auto, cpu, gpu\"\"\"\n",
        "\n",
        "    control_mode: Annotated[str, tyro.conf.arg(aliases=[\"-c\"])] = \"pd_joint_pos\"\n",
        "    \"\"\"Control mode (e.g. pd_joint_pos, pd_joint_vel, etc.)\"\"\"\n",
        "\n",
        "    keyframe: Annotated[Optional[str], tyro.conf.arg(aliases=[\"-k\"])] = None\n",
        "    \"\"\"Name of keyframe to view\"\"\"\n",
        "\n",
        "    shader: str = \"default\"\n",
        "    \"\"\"Shader used for rendering (default, rt, rt-fast)\"\"\"\n",
        "\n",
        "    keyframe_actions: bool = False\n",
        "    \"\"\"Use keyframe to control robot pose\"\"\"\n",
        "\n",
        "    random_actions: bool = False\n",
        "    \"\"\"Send random actions each step\"\"\"\n",
        "\n",
        "    none_actions: bool = False\n",
        "    \"\"\"Send no actions (manual GUI only)\"\"\"\n",
        "\n",
        "    zero_actions: bool = False\n",
        "    \"\"\"Send zero actions\"\"\"\n",
        "\n",
        "    sim_freq: int = 100\n",
        "    \"\"\"Simulation frequency\"\"\"\n",
        "\n",
        "    control_freq: int = 20\n",
        "    \"\"\"Control frequency\"\"\"\n",
        "\n",
        "    obs_mode: str = \"none\"\n",
        "    \"\"\"Observation mode\"\"\"\n",
        "\n",
        "    reward_mode: Optional[str] = None\n",
        "    \"\"\"Reward mode\"\"\"\n",
        "\n",
        "    render_mode: str = \"human\"\n",
        "    \"\"\"Render mode (rgb_array, human, etc.)\"\"\"\n",
        "\n",
        "    pause: Annotated[bool, tyro.conf.arg(aliases=[\"-p\"])] = False\n",
        "    \"\"\"Pause viewer on load\"\"\"\n",
        "\n",
        "    record_dir: Optional[str] = None\n",
        "    \"\"\"Directory to save recordings\"\"\"\n",
        "\n",
        "    quiet: bool = False\n",
        "    \"\"\"Disable verbose output\"\"\"\n",
        "\n",
        "    seed: Annotated[Optional[Union[int, List[int]]], tyro.conf.arg(aliases=[\"-s\"])] = None\n",
        "    \"\"\"Random seed or list of seeds\"\"\"\n",
        "\n",
        "    num_envs: Annotated[int, tyro.conf.arg(aliases=[\"-n\"])] = 1\n",
        "    \"\"\"Number of environments\"\"\"\n",
        "    plan_actions: bool = False\n",
        "@register_agent()\n",
        "class MyPanda(BaseAgent):\n",
        "    uid = \"my_panda\"\n",
        "    urdf_path = \"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.urdf\"\n",
        "    keyframes = dict(\n",
        "        standing=Keyframe(\n",
        "            # notice how we set the z position to be above 0, so the robot is not intersecting the ground\n",
        "            pose=sapien.Pose(p=[1.4, -1.5, 0.01], q=R.from_euler(\"z\", 90, degrees=True).as_quat().tolist()),\n",
        "            qpos=np.array([\n",
        "                # arm: 7 joints\n",
        "                -1.75, -0.35, -3.3, 1.4, 3.4, -1.1, 1.4,0.0, 0.0, 0.0, 0.0,\n",
        "                # gripper: 6 joints\n",
        "                0.04,  # finger_joint\n",
        "                0.04, 0.04, 0.04, 0.04, 0.04  # mimic joints\n",
        "            ])\n",
        "        )\n",
        "    )\n",
        "    urdf_config = dict(\n",
        "        _materials=dict(\n",
        "            gripper=dict(static_friction=2.0, dynamic_friction=2.0, restitution=0.0)\n",
        "        ),\n",
        "        link=dict(\n",
        "            left_inner_finger_pad=dict(\n",
        "                material=\"gripper\", patch_radius=0.1, min_patch_radius=0.1\n",
        "            ),\n",
        "            right_inner_finger_pad=dict(\n",
        "                material=\"gripper\", patch_radius=0.1, min_patch_radius=0.1\n",
        "            ),\n",
        "            left_inner_finger=dict(material=\"gripper\"),\n",
        "            right_inner_finger=dict(material=\"gripper\"),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def _controller_configs(self):\n",
        "        arm_joint_names = [\n",
        "            \"joint_1\", \"joint_2\", \"joint_3\",\n",
        "            \"joint_4\", \"joint_5\", \"joint_6\", \"joint_7\",\n",
        "        ]\n",
        "        gripper_joint_names = [\"finger_joint\"]\n",
        "                # ✅ 添加 wheel joints\n",
        "        wheel_joint_names = [\n",
        "            \"joint_LF01\", \"joint_LB01\", \"joint_RF01\", \"joint_RB01\",\n",
        "        ]\n",
        "\n",
        "        arm_pd_joint_pos = PDJointPosControllerConfig(\n",
        "            arm_joint_names,\n",
        "            lower=[-3.14] * len(arm_joint_names),\n",
        "            upper=[3.14] * len(arm_joint_names),\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "            normalize_action=False,\n",
        "        )\n",
        "        arm_pd_joint_delta_pos = PDJointPosControllerConfig(\n",
        "            arm_joint_names,\n",
        "            lower=[-0.1] * len(arm_joint_names),\n",
        "            upper=[0.1] * len(arm_joint_names),\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "            use_delta=True,\n",
        "        )\n",
        "        gripper_pd_joint_pos = PDJointPosMimicControllerConfig(\n",
        "            gripper_joint_names,\n",
        "            lower=[-0.01],\n",
        "            upper=[0.04],\n",
        "            stiffness=1000,\n",
        "            damping=100,\n",
        "            force_limit=100,\n",
        "        )\n",
        "        wheel_pd_joint_pos = PDJointPosControllerConfig(\n",
        "            wheel_joint_names,\n",
        "            lower=[-100] * len(wheel_joint_names),\n",
        "            upper=[100] * len(wheel_joint_names),\n",
        "            stiffness=1000,\n",
        "            damping=50,\n",
        "            force_limit=50,\n",
        "            normalize_action=False,\n",
        "        )\n",
        "\n",
        "        return deepcopy({\n",
        "            \"pd_joint_delta_pos\": {\n",
        "                \"arm\": arm_pd_joint_delta_pos,\n",
        "                \"gripper\": gripper_pd_joint_pos,\n",
        "                \"wheels\": wheel_pd_joint_pos,\n",
        "            },\n",
        "            \"pd_joint_pos\": {\n",
        "                \"arm\": arm_pd_joint_pos,\n",
        "                \"gripper\": gripper_pd_joint_pos,\n",
        "                \"wheels\": wheel_pd_joint_pos,\n",
        "            },\n",
        "        })\n",
        "\n",
        "\n",
        "    # wheel_joint_names = [\n",
        "    #     \"joint_LF01\",\n",
        "    #     \"joint_LB01\",\n",
        "    #     \"joint_RF01\",\n",
        "    #     \"joint_RB01\"\n",
        "    # ]\n",
        "    #     # 设定控制参数（可根据实际需要修改）\n",
        "    # wheel_stiffness = 1000\n",
        "    # wheel_damping = 100\n",
        "    # wheel_force_limit = 100\n",
        "\n",
        "\n",
        "\n",
        "# ✅ 主函数\n",
        "def main():\n",
        "    args = tyro.cli(Args)\n",
        "\n",
        "    env = gym.make(\n",
        "        args.env_id,\n",
        "        obs_mode=args.obs_mode,\n",
        "        reward_mode=args.reward_mode,\n",
        "        enable_shadow=True,\n",
        "        control_mode=args.control_mode,\n",
        "        robot_uids=args.robot_uids,\n",
        "        sensor_configs={\"shader_pack\": args.shader},\n",
        "        human_render_camera_configs={\"shader_pack\": args.shader},\n",
        "        viewer_camera_configs={\"shader_pack\": args.shader},\n",
        "        render_mode=args.render_mode,\n",
        "        sim_config=dict(sim_freq=args.sim_freq, control_freq=args.control_freq),\n",
        "        sim_backend=args.sim_backend,\n",
        "    )\n",
        "\n",
        "    env.reset(seed=args.seed or 0)\n",
        "    env: BaseEnv = env.unwrapped\n",
        "\n",
        "    print(f\"✅ Selected robot: {args.robot_uids}, control mode: {args.control_mode}\")\n",
        "    print(f\"🔑 Available keyframes: {list(env.agent.keyframes.keys())}\")\n",
        "    import sapien\n",
        "    from mani_skill.envs.scene import ManiSkillScene\n",
        "    from mani_skill.utils.building import URDFLoader\n",
        "    loader = URDFLoader()\n",
        "    loader.set_scene(ManiSkillScene())\n",
        "    robot = loader.load(\"/home/shiqintong/Downloads/wheelchair_description/urdf/inte.urdf\")\n",
        "    print(robot.active_joints_map.keys())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 初始化位姿\n",
        "    kf = None\n",
        "    if env.agent.keyframes:\n",
        "        kf_name = args.keyframe or next(iter(env.agent.keyframes))\n",
        "        kf = env.agent.keyframes[kf_name]\n",
        "        env.agent.robot.set_pose(kf.pose)\n",
        "        if kf.qpos is not None:\n",
        "            env.agent.robot.set_qpos(kf.qpos)\n",
        "        if kf.qvel is not None:\n",
        "            env.agent.robot.set_qvel(kf.qvel)\n",
        "        print(f\"📌 Viewing keyframe: {kf_name}\")\n",
        "\n",
        "    if env.gpu_sim_enabled:\n",
        "        env.scene._gpu_apply_all()\n",
        "        env.scene.px.gpu_update_articulation_kinematics()\n",
        "        env.scene._gpu_fetch_all()\n",
        "\n",
        "    viewer = env.render()\n",
        "    viewer.paused = args.pause\n",
        "\n",
        "    # ✅ 设置 robot base pose\n",
        "    from scipy.spatial.transform import Rotation as R\n",
        "    base_pose = sapien.Pose(\n",
        "        [1.4, -3.5, 0.01],  # 可按实际情况调整\n",
        "        R.from_euler(\"z\", 90, degrees=True).as_quat().tolist()\n",
        "    )\n",
        "\n",
        "    planner = None\n",
        "    if args.plan_actions:\n",
        "        from transforms3d.euler import euler2quat\n",
        "        from transforms3d.euler import mat2euler\n",
        "\n",
        "        planner = MyPandaMotionPlanningSolver(\n",
        "            env,\n",
        "            debug=False,\n",
        "            vis=True,\n",
        "            base_pose=env.agent.robot.pose, # 你也可指定别的\n",
        "            visualize_target_grasp_pose=False,\n",
        "            print_env_info=True,\n",
        "            joint_acc_limits=0.5,\n",
        "            joint_vel_limits=0.5,\n",
        "        )\n",
        "    arm_qpos = env.agent.robot.get_qpos()[0, :7].cpu().numpy()\n",
        "    gripper_state = 0.8\n",
        "        # 初始化轮子状态\n",
        "    wheel_qpos = {\n",
        "        \"joint_LF01\": 0.0,\n",
        "        \"joint_LB01\": 0.0,\n",
        "        \"joint_RF01\": 0.0,\n",
        "        \"joint_RB01\": 0.0,\n",
        "    }\n",
        "\n",
        "    # 获取轮子 joint index\n",
        "    wheel_joint_ids = {\n",
        "        name: env.agent.robot.active_joints_map[name].index\n",
        "        for name in wheel_qpos\n",
        "    }\n",
        "    while True:\n",
        "        viewer = env.render()\n",
        "\n",
        "        if args.random_actions:\n",
        "            # 控制机械臂\n",
        "            if viewer.window.key_down(\"b\"):\n",
        "                arm_qpos[1] += 0.05\n",
        "            if viewer.window.key_down(\"n\"):\n",
        "                arm_qpos[1] -= 0.05\n",
        "            if viewer.window.key_down(\"z\"):\n",
        "                arm_qpos[2] += 0.05\n",
        "            if viewer.window.key_down(\"x\"):\n",
        "                arm_qpos[2] -= 0.05\n",
        "            if viewer.window.key_down(\"c\"):\n",
        "                arm_qpos[3] += 0.05\n",
        "            if viewer.window.key_down(\"v\"):\n",
        "                arm_qpos[3] -= 0.05\n",
        "            if viewer.window.key_down(\"m\"):\n",
        "                arm_qpos[4] += 0.05\n",
        "            if viewer.window.key_down(\"f\"):\n",
        "                arm_qpos[4] -= 0.05\n",
        "            if viewer.window.key_down(\"g\"):\n",
        "                arm_qpos[5] += 0.05\n",
        "            if viewer.window.key_down(\"h\"):\n",
        "                arm_qpos[5] -= 0.05\n",
        "            if viewer.window.key_down(\"r\"):\n",
        "                arm_qpos[6] += 0.05\n",
        "            if viewer.window.key_down(\"t\"):\n",
        "                arm_qpos[6] -= 0.05\n",
        "            if viewer.window.key_down(\"up\"):\n",
        "                arm_qpos[0] += 0.05\n",
        "            if viewer.window.key_down(\"down\"):\n",
        "                arm_qpos[0] -= 0.05\n",
        "\n",
        "            # 控制夹爪\n",
        "            if viewer.window.key_down(\"o\"):\n",
        "                gripper_state = 0.1\n",
        "            if viewer.window.key_down(\"p\"):\n",
        "                gripper_state = 0.0\n",
        "\n",
        "            # 控制轮子\n",
        "            delta = 0.1\n",
        "            if viewer.window.key_down(\"i\"):  # 前进\n",
        "                for k in wheel_qpos:\n",
        "                    wheel_qpos[k] += delta\n",
        "            if viewer.window.key_down(\"k\"):  # 后退\n",
        "                for k in wheel_qpos:\n",
        "                    wheel_qpos[k] -= delta\n",
        "            if viewer.window.key_down(\"j\"):  # 左转\n",
        "                wheel_qpos[\"joint_LF01\"] -= delta\n",
        "                wheel_qpos[\"joint_LB01\"] -= delta\n",
        "                wheel_qpos[\"joint_RF01\"] += delta\n",
        "                wheel_qpos[\"joint_RB01\"] += delta\n",
        "            if viewer.window.key_down(\"l\"):  # 右转\n",
        "                wheel_qpos[\"joint_LF01\"] += delta\n",
        "                wheel_qpos[\"joint_LB01\"] += delta\n",
        "                wheel_qpos[\"joint_RF01\"] -= delta\n",
        "                wheel_qpos[\"joint_RB01\"] -= delta\n",
        "\n",
        "            # 构造总的 action (7 + 1 + 4 = 12)\n",
        "            action = np.hstack([\n",
        "                arm_qpos,               # 7 joints\n",
        "                [gripper_state],        # 1 gripper joint\n",
        "                [wheel_qpos[\"joint_LF01\"]],\n",
        "                [wheel_qpos[\"joint_LB01\"]],\n",
        "                [wheel_qpos[\"joint_RF01\"]],\n",
        "                [wheel_qpos[\"joint_RB01\"]],\n",
        "            ])[None, :]  # shape: (1, 12)\n",
        "            # else:\n",
        "            #     print(\"⚠️ 当前控制器不是 DictController，请手动拼接 action！\")\n",
        "            #     action = np.hstack([\n",
        "            #         arm_qpos,\n",
        "            #         gripper_state,\n",
        "            #         wheel_qpos[\"joint_LF01\"],\n",
        "            #         wheel_qpos[\"joint_LB01\"],\n",
        "            #         wheel_qpos[\"joint_RF01\"],\n",
        "            #         wheel_qpos[\"joint_RB01\"],\n",
        "            #     ])[None, :]\n",
        "\n",
        "            env.step(action)\n",
        "            print(\"Current qpos:\", np.round(arm_qpos, 3))\n",
        "\n",
        "        elif args.none_actions:\n",
        "            env.step(None)\n",
        "        elif args.zero_actions:\n",
        "            env.step(np.zeros_like(env.action_space.sample()))\n",
        "        elif args.keyframe_actions:\n",
        "            if kf is not None:\n",
        "                if isinstance(env.agent.controller, DictController):\n",
        "                    env.step(env.agent.controller.from_qpos(kf.qpos))\n",
        "                else:\n",
        "                    env.step(kf.qpos)\n",
        "        elif args.plan_actions:\n",
        "            target_pose = sapien.Pose([0.0, 0.0, 0.0])\n",
        "            result = planner.move_to_pose_with_screw(target_pose, dry_run=True)\n",
        "            if result != -1:\n",
        "                planner.follow_path(result)\n",
        "\n",
        "    # while True:\n",
        "    #         viewer = env.render()\n",
        "\n",
        "    #         if args.random_actions:\n",
        "    #             # 控制机械臂\n",
        "    #             if viewer.window.key_down(\"b\"):\n",
        "    #                 arm_qpos[1] += 0.05\n",
        "    #             if viewer.window.key_down(\"n\"):\n",
        "    #                 arm_qpos[1] -= 0.05\n",
        "\n",
        "    #             # 控制夹爪\n",
        "    #             if viewer.window.key_down(\"o\"):\n",
        "    #                 gripper_state = 0.1\n",
        "    #             if viewer.window.key_down(\"p\"):\n",
        "    #                 gripper_state = 0.0\n",
        "\n",
        "    #             # 控制轮子\n",
        "    #             delta = 0.1\n",
        "    #             if viewer.window.key_down(\"i\"):\n",
        "    #                 for k in wheel_qpos:\n",
        "    #                     wheel_qpos[k] += delta\n",
        "    #             if viewer.window.key_down(\"k\"):\n",
        "    #                 for k in wheel_qpos:\n",
        "    #                     wheel_qpos[k] -= delta\n",
        "    #             if viewer.window.key_down(\"j\"):\n",
        "    #                 wheel_qpos[\"joint_LF01\"] -= delta\n",
        "    #                 wheel_qpos[\"joint_LB01\"] -= delta\n",
        "    #                 wheel_qpos[\"joint_RF01\"] += delta\n",
        "    #                 wheel_qpos[\"joint_RB01\"] += delta\n",
        "    #             if viewer.window.key_down(\"l\"):\n",
        "    #                 wheel_qpos[\"joint_LF01\"] += delta\n",
        "    #                 wheel_qpos[\"joint_LB01\"] += delta\n",
        "    #                 wheel_qpos[\"joint_RF01\"] -= delta\n",
        "    #                 wheel_qpos[\"joint_RB01\"] -= delta\n",
        "\n",
        "    #             # 构造总的 action\n",
        "    #             total_qpos = np.zeros(env.agent.robot.dof)\n",
        "\n",
        "    #             # 机械臂 7 joints + gripper\n",
        "    #             total_qpos[:7] = arm_qpos\n",
        "    #             total_qpos[7] = gripper_state\n",
        "\n",
        "    #             # 设置轮子 joint 的角度\n",
        "    #             for name, value in wheel_qpos.items():\n",
        "    #                 index = wheel_joint_ids[name]\n",
        "    #                 total_qpos[index] = value\n",
        "\n",
        "    #             action = total_qpos[None, :]  # (1, DOF)\n",
        "    #             env.step(action)\n",
        "\n",
        "    #         elif args.none_actions:\n",
        "    #             env.step(None)\n",
        "    #         elif args.zero_actions:\n",
        "    #             env.step(np.zeros_like(env.action_space.sample()))\n",
        "    #         elif args.keyframe_actions:\n",
        "    #             if kf is not None:\n",
        "    #                 if isinstance(env.agent.controller, DictController):\n",
        "    #                     env.step(env.agent.controller.from_qpos(kf.qpos))\n",
        "    #                 else:\n",
        "    #                     env.step(kf.qpos)\n",
        "    #         elif args.plan_actions:\n",
        "    #             # 示例路径规划\n",
        "    #             target_pose = sapien.Pose([0.0, 0.0, 0.0])\n",
        "    #             result = planner.move_to_pose_with_screw(target_pose, dry_run=True)\n",
        "    #             if result != -1:\n",
        "    #                 planner.follow_path(result)\n",
        "\n",
        "    # # ✅ 主循环\n",
        "    # while True:\n",
        "    #     if args.random_actions:\n",
        "    #         # full_qpos = env.agent.robot.get_qpos()[0].cpu().numpy()\n",
        "\n",
        "    #         # # 只取前 7 个 arm joints\n",
        "    #         # arm_qpos = full_qpos[:7]\n",
        "\n",
        "    #         # 键盘控制 joint_3 上下（索引 2）\n",
        "    #         if viewer.window.key_down(\"b\"):\n",
        "    #             arm_qpos[1] += 0.05  # joint_3 向上\n",
        "    #         if viewer.window.key_down(\"n\"):\n",
        "    #             arm_qpos[1] -= 0.05  # joint_3 向下\n",
        "\n",
        "\n",
        "    #         if viewer.window.key_down(\"o\"):\n",
        "    #             gripper_state = 0.1  # fully open\n",
        "    #         if viewer.window.key_down(\"p\"):\n",
        "    #             gripper_state = 0.0  # fully closed\n",
        "\n",
        "    #         # 构造 action\n",
        "    #         if env.control_mode == \"pd_joint_pos\":\n",
        "    #             action = np.hstack([arm_qpos, gripper_state])[None, :]  # (1, 8)\n",
        "    #         elif env.control_mode == \"pd_joint_pos_vel\":\n",
        "    #             qvel = np.zeros_like(arm_qpos)\n",
        "    #             action = np.hstack([arm_qpos, qvel, gripper_state])[None, :]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #         else:\n",
        "    #             print(\"⚠️ 当前控制模式未处理，使用随机动作代替\")\n",
        "    #             action = env.action_space.sample()\n",
        "\n",
        "    #         env.step(action)\n",
        "    #         # env.step(env.action_space.sample())\n",
        "\n",
        "    #         # env.step(env.action_space.sample())\n",
        "    #     elif args.none_actions:\n",
        "    #         env.step(None)\n",
        "    #     elif args.zero_actions:\n",
        "    #         env.step(np.zeros_like(env.action_space.sample()))\n",
        "    #     elif args.keyframe_actions:\n",
        "    #         if kf is not None:\n",
        "    #             if isinstance(env.agent.controller, DictController):\n",
        "    #                 env.step(env.agent.controller.from_qpos(kf.qpos))\n",
        "    #             else:\n",
        "    #                 env.step(kf.qpos)\n",
        "    #     elif args.plan_actions:\n",
        "    #         # 例如：规划一个目标末端位姿\n",
        "    #         target_pose = sapien.Pose([0.0, 0.0, 0.0])  # 随便写\n",
        "    #         result = planner.move_to_pose_with_screw(target_pose, dry_run=True)\n",
        "    #         if result == -1:\n",
        "    #             print(\"Plan failed!\")\n",
        "    #         else:\n",
        "    #             planner.follow_path(result)\n",
        "\n",
        "    #         # 这里演示一次就够，如果你要每帧做不同的规划，可以自己改逻辑\n",
        "\n",
        "    #     viewer = env.render()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "1DFg2cVit30k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#修改之后的planner 在mplib下 问题就是找到出问题的函数 日志+AI 一步一步推理得到关节有自撞 修改srdf去掉自撞\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import toppra as ta\n",
        "import toppra.algorithm as algo\n",
        "import toppra.constraint as constraint\n",
        "from transforms3d.quaternions import mat2quat, quat2mat\n",
        "\n",
        "from mplib.pymp import ArticulatedModel, PlanningWorld\n",
        "from mplib.pymp.planning import ompl\n",
        "\n",
        "\n",
        "class Planner:\n",
        "    \"\"\"Motion planner\"\"\"\n",
        "\n",
        "    # TODO(jigu): default joint vel and acc limits\n",
        "    # TODO(jigu): how does user link names and joint names are exactly used?\n",
        "    # constructor ankor\n",
        "    def __init__(\n",
        "        self,\n",
        "        urdf: str,\n",
        "        move_group: str,\n",
        "        srdf: str = \"\",\n",
        "        package_keyword_replacement: str = \"\",\n",
        "        user_link_names: Sequence[str] = [],\n",
        "        user_joint_names: Sequence[str] = [],\n",
        "        joint_vel_limits: Optional[Sequence[float] | np.ndarray] = None,\n",
        "        joint_acc_limits: Optional[Sequence[float] | np.ndarray] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # constructor ankor end\n",
        "        \"\"\"Motion planner for robots.\n",
        "\n",
        "        Args:\n",
        "            urdf: Unified Robot Description Format file.\n",
        "            user_link_names: names of links, the order matters.\n",
        "                If empty, all links will be used.\n",
        "            user_joint_names: names of the joints to plan.\n",
        "                If empty, all active joints will be used.\n",
        "            move_group: target link to move, usually the end-effector.\n",
        "            joint_vel_limits: maximum joint velocities for time parameterization,\n",
        "                which should have the same length as\n",
        "            joint_acc_limits: maximum joint accelerations for time parameterization,\n",
        "                which should have the same length as\n",
        "            srdf: Semantic Robot Description Format file.\n",
        "        References:\n",
        "            http://docs.ros.org/en/kinetic/api/moveit_tutorials/html/doc/urdf_srdf/urdf_srdf_tutorial.html\n",
        "\n",
        "        \"\"\"\n",
        "        if joint_vel_limits is None:\n",
        "            joint_vel_limits = []\n",
        "        if joint_acc_limits is None:\n",
        "            joint_acc_limits = []\n",
        "        self.urdf = urdf\n",
        "        if srdf == \"\" and os.path.exists(urdf.replace(\".urdf\", \".srdf\")):\n",
        "            self.srdf = urdf.replace(\".urdf\", \".srdf\")\n",
        "            print(f\"No SRDF file provided but found {self.srdf}\")\n",
        "\n",
        "        # replace package:// keyword if exists\n",
        "        urdf = self.replace_package_keyword(package_keyword_replacement)\n",
        "\n",
        "        self.robot = ArticulatedModel(\n",
        "            urdf,\n",
        "            srdf,\n",
        "            [0, 0, -9.81],\n",
        "            user_link_names,\n",
        "            user_joint_names,\n",
        "            convex=True,\n",
        "            verbose=False,\n",
        "        )\n",
        "        self.pinocchio_model = self.robot.get_pinocchio_model()\n",
        "        self.user_link_names = self.pinocchio_model.get_link_names()\n",
        "        self.user_joint_names = self.pinocchio_model.get_joint_names()\n",
        "\n",
        "        self.planning_world = PlanningWorld(\n",
        "            [self.robot],\n",
        "            [\"robot\"],\n",
        "            kwargs.get(\"normal_objects\", []),\n",
        "            kwargs.get(\"normal_object_names\", []),\n",
        "        )\n",
        "\n",
        "        if srdf == \"\":\n",
        "            self.generate_collision_pair()\n",
        "            self.robot.update_SRDF(self.srdf)\n",
        "\n",
        "        self.joint_name_2_idx = {}\n",
        "        for i, joint in enumerate(self.user_joint_names):\n",
        "            self.joint_name_2_idx[joint] = i\n",
        "        self.link_name_2_idx = {}\n",
        "        for i, link in enumerate(self.user_link_names):\n",
        "            self.link_name_2_idx[link] = i\n",
        "\n",
        "        assert (\n",
        "            move_group in self.user_link_names\n",
        "        ), f\"end-effector not found as one of the links in {self.user_link_names}\"\n",
        "        self.move_group = move_group\n",
        "        self.robot.set_move_group(self.move_group)\n",
        "        self.move_group_joint_indices = self.robot.get_move_group_joint_indices()\n",
        "\n",
        "        self.joint_types = self.pinocchio_model.get_joint_types()\n",
        "        self.joint_limits = np.concatenate(self.pinocchio_model.get_joint_limits())\n",
        "        self.joint_vel_limits = (\n",
        "            joint_vel_limits\n",
        "            if len(joint_vel_limits)\n",
        "            else np.ones(len(self.move_group_joint_indices))\n",
        "        )\n",
        "        self.joint_acc_limits = (\n",
        "            joint_acc_limits\n",
        "            if len(joint_acc_limits)\n",
        "            else np.ones(len(self.move_group_joint_indices))\n",
        "        )\n",
        "        self.move_group_link_id = self.link_name_2_idx[self.move_group]\n",
        "        assert len(self.joint_vel_limits) == len(self.joint_acc_limits), (\n",
        "            f\"length of joint_vel_limits ({len(self.joint_vel_limits)}) =/= \"\n",
        "            f\"length of joint_acc_limits ({len(self.joint_acc_limits)})\"\n",
        "        )\n",
        "        assert len(self.joint_vel_limits) == len(self.move_group_joint_indices), (\n",
        "            f\"length of joint_vel_limits ({len(self.joint_vel_limits)}) =/= \"\n",
        "            f\"length of move_group ({len(self.move_group_joint_indices)})\"\n",
        "        )\n",
        "        assert len(self.joint_vel_limits) <= len(self.joint_limits), (\n",
        "            f\"length of joint_vel_limits ({len(self.joint_vel_limits)}) > \"\n",
        "            f\"number of total joints ({len(self.joint_limits)})\"\n",
        "        )\n",
        "\n",
        "        self.planning_world = PlanningWorld([self.robot], [\"robot\"], [], [])\n",
        "        self.planner = ompl.OMPLPlanner(world=self.planning_world)\n",
        "\n",
        "    def replace_package_keyword(self, package_keyword_replacement):\n",
        "        \"\"\"\n",
        "        some ROS URDF files use package:// keyword to refer the package dir\n",
        "        replace it with the given string (default is empty)\n",
        "\n",
        "        Args:\n",
        "            package_keyword_replacement: the string to replace 'package://' keyword\n",
        "        \"\"\"\n",
        "        rtn_urdf = self.urdf\n",
        "        with open(self.urdf) as in_f:\n",
        "            content = in_f.read()\n",
        "            if \"package://\" in content:\n",
        "                rtn_urdf = self.urdf.replace(\".urdf\", \"_package_keyword_replaced.urdf\")\n",
        "                content = content.replace(\"package://\", package_keyword_replacement)\n",
        "                if not os.path.exists(rtn_urdf):\n",
        "                    with open(rtn_urdf, \"w\") as out_f:\n",
        "                        out_f.write(content)\n",
        "        return rtn_urdf\n",
        "\n",
        "    def generate_collision_pair(self, sample_time=1000000, echo_freq=100000):\n",
        "        \"\"\"\n",
        "        We read the srdf file to get the link pairs that should not collide.\n",
        "        If not provided, we need to randomly sample configurations\n",
        "        to find the link pairs that will always collide.\n",
        "        \"\"\"\n",
        "        print(\n",
        "            \"Since no SRDF file is provided. We will first detect link pairs that will\"\n",
        "            \" always collide. This may take several minutes.\"\n",
        "        )\n",
        "        n_link = len(self.user_link_names)\n",
        "        cnt = np.zeros((n_link, n_link), dtype=np.int32)\n",
        "        for i in range(sample_time):\n",
        "            qpos = self.pinocchio_model.get_random_configuration()\n",
        "            self.robot.set_qpos(qpos, True)\n",
        "            collisions = self.planning_world.collide_full()\n",
        "            for collision in collisions:\n",
        "                u = self.link_name_2_idx[collision.link_name1]\n",
        "                v = self.link_name_2_idx[collision.link_name2]\n",
        "                cnt[u][v] += 1\n",
        "            if i % echo_freq == 0:\n",
        "                print(\"Finish %.1f%%!\" % (i * 100 / sample_time))\n",
        "\n",
        "        import xml.etree.ElementTree as ET\n",
        "        from xml.dom import minidom\n",
        "\n",
        "        root = ET.Element(\"robot\")\n",
        "        robot_name = self.urdf.split(\"/\")[-1].split(\".\")[0]\n",
        "        root.set(\"name\", robot_name)\n",
        "        self.srdf = self.urdf.replace(\".urdf\", \".srdf\")\n",
        "\n",
        "        for i in range(n_link):\n",
        "            for j in range(n_link):\n",
        "                if cnt[i][j] == sample_time:\n",
        "                    link1 = self.user_link_names[i]\n",
        "                    link2 = self.user_link_names[j]\n",
        "                    print(\n",
        "                        f\"Ignore collision pair: ({link1}, {link2}), \"\n",
        "                        \"reason: always collide\"\n",
        "                    )\n",
        "                    collision = ET.SubElement(root, \"disable_collisions\")\n",
        "                    collision.set(\"link1\", link1)\n",
        "                    collision.set(\"link2\", link2)\n",
        "                    collision.set(\"reason\", \"Default\")\n",
        "        with open(self.srdf, \"w\") as srdf_file:\n",
        "            srdf_file.write(\n",
        "                minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"    \")\n",
        "            )\n",
        "            srdf_file.close()\n",
        "        print(\"Saving the SRDF file to %s\" % self.srdf)\n",
        "\n",
        "    def distance_6D(self, p1, q1, p2, q2):\n",
        "        \"\"\"\n",
        "        compute the distance between two poses\n",
        "\n",
        "        Args:\n",
        "            p1: position of pose 1\n",
        "            q1: quaternion of pose 1\n",
        "            p2: position of pose 2\n",
        "            q2: quaternion of pose 2\n",
        "        \"\"\"\n",
        "        return np.linalg.norm(p1 - p2) + min(\n",
        "            np.linalg.norm(q1 - q2), np.linalg.norm(q1 + q2)\n",
        "        )\n",
        "\n",
        "    def wrap_joint_limit(self, q) -> bool:\n",
        "        \"\"\"\n",
        "        Checks if the joint configuration is within the joint limits.\n",
        "        For revolute joints, the joint angle is wrapped to be within [q_min, q_min+2*pi)\n",
        "\n",
        "        Args:\n",
        "            q: joint configuration, angles of revolute joints might be modified\n",
        "\n",
        "        Returns:\n",
        "            True if q can be wrapped to be within the joint limits\n",
        "        \"\"\"\n",
        "        n = len(q)\n",
        "        flag = True\n",
        "        for i in range(n):\n",
        "            if self.joint_types[i].startswith(\"JointModelR\"):\n",
        "                if -1e-3 <= q[i] - self.joint_limits[i][0] < 0:\n",
        "                    continue\n",
        "                q[i] -= (\n",
        "                    2 * np.pi * np.floor((q[i] - self.joint_limits[i][0]) / (2 * np.pi))\n",
        "                )\n",
        "                if q[i] > self.joint_limits[i][1] + 1e-3:\n",
        "                    flag = False\n",
        "            else:\n",
        "                if (\n",
        "                    q[i] < self.joint_limits[i][0] - 1e-3\n",
        "                    or q[i] > self.joint_limits[i][1] + 1e-3\n",
        "                ):\n",
        "                    flag = False\n",
        "        return flag\n",
        "\n",
        "    def pad_qpos(self, qpos, articulation=None):\n",
        "        \"\"\"\n",
        "        if the user does not provide the full qpos but only the move_group joints,\n",
        "        pad the qpos with the rest of the joints\n",
        "        \"\"\"\n",
        "        if len(qpos) == len(self.move_group_joint_indices):\n",
        "            tmp = (\n",
        "                articulation.get_qpos()\n",
        "                if articulation is not None\n",
        "                else self.robot.get_qpos()\n",
        "            )\n",
        "            tmp[: len(qpos)] = qpos\n",
        "            qpos = tmp\n",
        "\n",
        "        assert len(qpos) == len(self.joint_limits), (\n",
        "            f\"length of qpos ({len(qpos)}) =/= \"\n",
        "            f\"number of total joints ({len(self.joint_limits)})\"\n",
        "        )\n",
        "\n",
        "        return qpos\n",
        "\n",
        "    def check_for_collision(\n",
        "        self,\n",
        "        collision_function,\n",
        "        articulation: Optional[ArticulatedModel] = None,\n",
        "        qpos: Optional[np.ndarray] = None,\n",
        "    ) -> list:\n",
        "        \"\"\"helper function to check for collision\"\"\"\n",
        "        # handle no user input\n",
        "        if articulation is None:\n",
        "            articulation = self.robot\n",
        "        if qpos is None:\n",
        "            qpos = articulation.get_qpos()\n",
        "        qpos = self.pad_qpos(qpos, articulation)\n",
        "\n",
        "        # first save the current qpos\n",
        "        old_qpos = articulation.get_qpos()\n",
        "        # set robot to new qpos\n",
        "        articulation.set_qpos(qpos, True)\n",
        "        # find the index of the articulation inside the array\n",
        "        idx = self.planning_world.get_articulations().index(articulation)\n",
        "        # check for self-collision\n",
        "        collisions = collision_function(idx)\n",
        "        # reset qpos\n",
        "        articulation.set_qpos(old_qpos, True)\n",
        "        return collisions\n",
        "\n",
        "    def check_for_self_collision(\n",
        "        self,\n",
        "        articulation: Optional[ArticulatedModel] = None,\n",
        "        qpos: Optional[np.ndarray] = None,\n",
        "    ) -> list:\n",
        "        \"\"\"Check if the robot is in self-collision.\n",
        "\n",
        "        Args:\n",
        "            articulation: robot model. if none will be self.robot\n",
        "            qpos: robot configuration. if none will be the current pose\n",
        "\n",
        "        Returns:\n",
        "            A list of collisions.\n",
        "        \"\"\"\n",
        "        return self.check_for_collision(\n",
        "            self.planning_world.self_collide, articulation, qpos\n",
        "        )\n",
        "\n",
        "    def check_for_env_collision(\n",
        "        self,\n",
        "        articulation: Optional[ArticulatedModel] = None,\n",
        "        qpos: Optional[np.ndarray] = None,\n",
        "        with_point_cloud=False,\n",
        "        use_attach=False,\n",
        "    ) -> list:\n",
        "        \"\"\"Check if the robot is in collision with the environment\n",
        "\n",
        "        Args:\n",
        "            articulation: robot model. if none will be self.robot\n",
        "            qpos: robot configuration. if none will be the current pose\n",
        "            with_point_cloud: whether to check collision against point cloud\n",
        "            use_attach: whether to include the object attached to the end effector\n",
        "                in collision checking\n",
        "        Returns:\n",
        "            A list of collisions.\n",
        "        \"\"\"\n",
        "        # store previous results\n",
        "        prev_use_point_cloud = self.planning_world.use_point_cloud\n",
        "        prev_use_attach = self.planning_world.use_attach\n",
        "        self.planning_world.set_use_point_cloud(with_point_cloud)\n",
        "        self.planning_world.set_use_attach(use_attach)\n",
        "\n",
        "        results = self.check_for_collision(\n",
        "            self.planning_world.collide_with_others, articulation, qpos\n",
        "        )\n",
        "\n",
        "        # restore\n",
        "        self.planning_world.set_use_point_cloud(prev_use_point_cloud)\n",
        "        self.planning_world.set_use_attach(prev_use_attach)\n",
        "        return results\n",
        "\n",
        "    def IK(self, goal_pose, start_qpos, mask=None, n_init_qpos=20, threshold=1e-3):\n",
        "        \"\"\"\n",
        "        Inverse kinematics\n",
        "\n",
        "        Args:\n",
        "            goal_pose: [x,y,z,qw,qx,qy,qz] pose of the goal\n",
        "            start_qpos: initial configuration\n",
        "            mask: if the value at a given index is True,\n",
        "                the joint is *not* used in the IK\n",
        "            n_init_qpos: number of random initial configurations\n",
        "            threshold: threshold for the distance between the goal pose and\n",
        "                the result pose\n",
        "        \"\"\"\n",
        "\n",
        "        if mask is None:\n",
        "            mask = []\n",
        "        index = self.link_name_2_idx[self.move_group]\n",
        "        min_dis = 1e9\n",
        "        idx = self.move_group_joint_indices\n",
        "        qpos0 = np.copy(start_qpos)\n",
        "        results = []\n",
        "        self.robot.set_qpos(start_qpos, True)\n",
        "        for _ in range(n_init_qpos):\n",
        "            ik_results = self.pinocchio_model.compute_IK_CLIK(\n",
        "                index, goal_pose, start_qpos, mask\n",
        "            )\n",
        "            flag = self.wrap_joint_limit(ik_results[0])  # will wrap revolute joints\n",
        "\n",
        "            # check collision\n",
        "            self.planning_world.set_qpos_all(ik_results[0][idx])\n",
        "            if len(self.planning_world.collide_full()) != 0:\n",
        "                flag = False\n",
        "\n",
        "            if flag:\n",
        "                self.pinocchio_model.compute_forward_kinematics(ik_results[0])\n",
        "                new_pose = self.pinocchio_model.get_link_pose(index)\n",
        "                tmp_dis = self.distance_6D(\n",
        "                    goal_pose[:3], goal_pose[3:], new_pose[:3], new_pose[3:]\n",
        "                )\n",
        "                if tmp_dis < min_dis:\n",
        "                    min_dis = tmp_dis\n",
        "                if tmp_dis < threshold:\n",
        "                    result = ik_results[0]\n",
        "                    unique = True\n",
        "                    for j in range(len(results)):\n",
        "                        if np.linalg.norm(results[j][idx] - result[idx]) < 0.1:\n",
        "                            unique = False\n",
        "                    if unique:\n",
        "                        results.append(result)\n",
        "            start_qpos = self.pinocchio_model.get_random_configuration()\n",
        "            mask_len = len(mask)\n",
        "            if mask_len > 0:\n",
        "                for j in range(mask_len):\n",
        "                    if mask[j]:\n",
        "                        start_qpos[j] = qpos0[j]\n",
        "        if len(results) != 0:\n",
        "            status = \"Success\"\n",
        "        elif min_dis != 1e9:\n",
        "            status = \"IK Failed! Distance {:f} is greater than threshold {:f}.\".format(\n",
        "                min_dis,\n",
        "                threshold,\n",
        "            )\n",
        "        else:\n",
        "            status = \"IK Failed! Cannot find valid solution.\"\n",
        "        return status, results\n",
        "\n",
        "    def TOPP(self, path, step=0.1, verbose=False):\n",
        "        \"\"\"\n",
        "        Time-Optimal Path Parameterization\n",
        "\n",
        "        Args:\n",
        "            path: numpy array of shape (n, dof)\n",
        "            step: step size for the discretization\n",
        "            verbose: if True, will print the log of TOPPRA\n",
        "        \"\"\"\n",
        "\n",
        "        N_samples = path.shape[0]\n",
        "        dof = path.shape[1]\n",
        "        assert dof == len(self.joint_vel_limits)\n",
        "        assert dof == len(self.joint_acc_limits)\n",
        "        ss = np.linspace(0, 1, N_samples)\n",
        "        path = ta.SplineInterpolator(ss, path)\n",
        "        pc_vel = constraint.JointVelocityConstraint(self.joint_vel_limits)\n",
        "        pc_acc = constraint.JointAccelerationConstraint(self.joint_acc_limits)\n",
        "        instance = algo.TOPPRA(\n",
        "            [pc_vel, pc_acc], path, parametrizer=\"ParametrizeConstAccel\"\n",
        "        )\n",
        "        jnt_traj = instance.compute_trajectory()\n",
        "        if jnt_traj is None:\n",
        "            raise RuntimeError(\"Fail to parameterize path\")\n",
        "        ts_sample = np.linspace(0, jnt_traj.duration, int(jnt_traj.duration / step))\n",
        "        qs_sample = jnt_traj(ts_sample)\n",
        "        qds_sample = jnt_traj(ts_sample, 1)\n",
        "        qdds_sample = jnt_traj(ts_sample, 2)\n",
        "        return ts_sample, qs_sample, qds_sample, qdds_sample, jnt_traj.duration\n",
        "\n",
        "    def update_point_cloud(self, pc, radius=1e-3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pc: numpy array of shape (n, 3)\n",
        "            radius: radius of each point. This gives a buffer around each point\n",
        "                that planner will avoid\n",
        "        \"\"\"\n",
        "        self.planning_world.update_point_cloud(pc, radius)\n",
        "\n",
        "    def update_attached_tool(self, fcl_collision_geometry, pose, link_id=-1):\n",
        "        \"\"\"helper function to update the attached tool\"\"\"\n",
        "        if link_id == -1:\n",
        "            link_id = self.move_group_link_id\n",
        "        self.planning_world.update_attached_tool(fcl_collision_geometry, link_id, pose)\n",
        "\n",
        "    def update_attached_sphere(self, radius, pose, link_id=-1):\n",
        "        \"\"\"\n",
        "        attach a sphere to some link\n",
        "\n",
        "        Args:\n",
        "            radius: radius of the sphere\n",
        "            pose: [x,y,z,qw,qx,qy,qz] pose of the sphere\n",
        "            link_id: if not provided, the end effector will be the target.\n",
        "        \"\"\"\n",
        "        if link_id == -1:\n",
        "            link_id = self.move_group_link_id\n",
        "        self.planning_world.update_attached_sphere(radius, link_id, pose)\n",
        "\n",
        "    def update_attached_box(self, size, pose, link_id=-1):\n",
        "        \"\"\"\n",
        "        attach a box to some link\n",
        "\n",
        "        Args:\n",
        "            size: [x,y,z] size of the box\n",
        "            pose: [x,y,z,qw,qx,qy,qz] pose of the box\n",
        "            link_id: if not provided, the end effector will be the target.\n",
        "        \"\"\"\n",
        "        if link_id == -1:\n",
        "            link_id = self.move_group_link_id\n",
        "        self.planning_world.update_attached_box(size, link_id, pose)\n",
        "\n",
        "    def update_attached_mesh(self, mesh_path, pose, link_id=-1):\n",
        "        \"\"\"\n",
        "        attach a mesh to some link\n",
        "\n",
        "        Args:\n",
        "            mesh_path: path to the mesh\n",
        "            pose: [x,y,z,qw,qx,qy,qz] pose of the mesh\n",
        "            link_id: if not provided, the end effector will be the target.\n",
        "        \"\"\"\n",
        "        if link_id == -1:\n",
        "            link_id = self.move_group_link_id\n",
        "        self.planning_world.update_attached_mesh(mesh_path, link_id, pose)\n",
        "\n",
        "    def set_base_pose(self, pose):\n",
        "        \"\"\"\n",
        "        tell the planner where the base of the robot is w.r.t the world frame\n",
        "\n",
        "        Args:\n",
        "            pose: [x,y,z,qw,qx,qy,qz] pose of the base\n",
        "        \"\"\"\n",
        "        self.robot.set_base_pose(pose)\n",
        "\n",
        "    def set_normal_object(self, name, collision_object):\n",
        "        \"\"\"adds or updates a non-articulated collision object in the scene\"\"\"\n",
        "        self.planning_world.set_normal_object(name, collision_object)\n",
        "\n",
        "    def remove_normal_object(self, name):\n",
        "        \"\"\"returns true if the object was removed, false if it was not found\"\"\"\n",
        "        return self.planning_world.remove_normal_object(name)\n",
        "\n",
        "    def plan_qpos_to_qpos(\n",
        "        self,\n",
        "        goal_qposes: list,\n",
        "        current_qpos,\n",
        "        time_step=0.1,\n",
        "        rrt_range=0.1,\n",
        "        planning_time=1,\n",
        "        fix_joint_limits=True,\n",
        "        use_point_cloud=False,\n",
        "        use_attach=False,\n",
        "        planner_name=\"RRTConnect\",\n",
        "        no_simplification=False,\n",
        "        constraint_function=None,\n",
        "        constraint_jacobian=None,\n",
        "        constraint_tolerance=1e-3,\n",
        "        fixed_joint_indices=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        plan a path from a specified joint position to a goal pose\n",
        "\n",
        "        Args:\n",
        "            goal_pose: 7D pose of the end-effector [x,y,z,qw,qx,qy,qz]\n",
        "            current_qpos: current joint configuration (either full or move_group joints)\n",
        "            mask: mask for IK. When set, the IK will leave certain joints out of\n",
        "                planning\n",
        "            time_step: time step for TOPP\n",
        "            rrt_range: step size for RRT\n",
        "            planning_time: time limit for RRT\n",
        "            fix_joint_limits: if True, will clip the joint configuration to be within\n",
        "                the joint limits\n",
        "            use_point_cloud: if True, will use the point cloud to avoid collision\n",
        "            use_attach: if True, will consider the attached tool collision when planning\n",
        "            planner_name: planner name pick from {\"RRTConnect\", \"RRTstar\"}\n",
        "            no_simplification: if true, will not simplify the path. constraint planning\n",
        "                does not support simplification\n",
        "            constraint_function: evals to 0 when constraint is satisfied\n",
        "            constraint_jacobian: jacobian of constraint_function\n",
        "            constraint_tolerance: tolerance for constraint_function\n",
        "            fixed_joint_indices: list of indices of joints that are fixed during\n",
        "                planning\n",
        "            verbose: if True, will print the log of OMPL and TOPPRA\n",
        "        \"\"\"\n",
        "        if fixed_joint_indices is None:\n",
        "            fixed_joint_indices = []\n",
        "        self.planning_world.set_use_point_cloud(use_point_cloud)\n",
        "        self.planning_world.set_use_attach(use_attach)\n",
        "        n = current_qpos.shape[0]\n",
        "        if fix_joint_limits:\n",
        "            for i in range(n):\n",
        "                if current_qpos[i] < self.joint_limits[i][0]:\n",
        "                    current_qpos[i] = self.joint_limits[i][0] + 1e-3\n",
        "                if current_qpos[i] > self.joint_limits[i][1]:\n",
        "                    current_qpos[i] = self.joint_limits[i][1] - 1e-3\n",
        "\n",
        "        current_qpos = self.pad_qpos(current_qpos)\n",
        "\n",
        "        self.robot.set_qpos(current_qpos, True)\n",
        "        collisions = self.planning_world.collide_full()\n",
        "        if len(collisions) != 0:\n",
        "            print(\"Invalid start state!\")\n",
        "            for collision in collisions:\n",
        "                print(f\"{collision.link_name1} and {collision.link_name2} collide!\")\n",
        "\n",
        "        idx = self.move_group_joint_indices\n",
        "\n",
        "        goal_qpos_ = [goal_qposes[i][idx] for i in range(len(goal_qposes))]\n",
        "\n",
        "        fixed_joints = set()\n",
        "        for joint_idx in fixed_joint_indices:\n",
        "            fixed_joints.add(ompl.FixedJoint(0, joint_idx, current_qpos[joint_idx]))\n",
        "\n",
        "        assert len(current_qpos[idx]) == len(goal_qpos_[0])\n",
        "        status, path = self.planner.plan(\n",
        "            current_qpos[idx],\n",
        "            goal_qpos_,\n",
        "            range=rrt_range,\n",
        "            time=planning_time,\n",
        "            fixed_joints=fixed_joints,\n",
        "            planner_name=planner_name,\n",
        "            no_simplification=no_simplification,\n",
        "            constraint_function=constraint_function,\n",
        "            constraint_jacobian=constraint_jacobian,\n",
        "            constraint_tolerance=constraint_tolerance,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "\n",
        "        if status == \"Exact solution\":\n",
        "            if verbose:\n",
        "                ta.setup_logging(\"INFO\")\n",
        "            else:\n",
        "                ta.setup_logging(\"WARNING\")\n",
        "            times, pos, vel, acc, duration = self.TOPP(path, time_step)\n",
        "            return {\n",
        "                \"status\": \"Success\",\n",
        "                \"time\": times,\n",
        "                \"position\": pos,\n",
        "                \"velocity\": vel,\n",
        "                \"acceleration\": acc,\n",
        "                \"duration\": duration,\n",
        "            }\n",
        "        else:\n",
        "            return {\"status\": \"RRT Failed. %s\" % status}\n",
        "\n",
        "    def transform_goal_to_wrt_base(self, goal_pose):\n",
        "        base_pose = self.robot.get_base_pose()\n",
        "        base_tf = np.eye(4)\n",
        "        base_tf[0:3, 3] = base_pose[:3]\n",
        "        base_tf[0:3, 0:3] = quat2mat(base_pose[3:])\n",
        "        goal_tf = np.eye(4)\n",
        "        goal_tf[0:3, 3] = goal_pose[:3]\n",
        "        goal_tf[0:3, 0:3] = quat2mat(goal_pose[3:])\n",
        "        goal_tf = np.linalg.inv(base_tf).dot(goal_tf)\n",
        "        new_goal_pose = np.zeros(7)\n",
        "        new_goal_pose[:3] = goal_tf[0:3, 3]\n",
        "        new_goal_pose[3:] = mat2quat(goal_tf[0:3, 0:3])\n",
        "        return new_goal_pose\n",
        "\n",
        "    def plan_qpos_to_pose(\n",
        "        self,\n",
        "        goal_pose,\n",
        "        current_qpos,\n",
        "        mask=None,\n",
        "        time_step=0.1,\n",
        "        rrt_range=0.1,\n",
        "        planning_time=1,\n",
        "        fix_joint_limits=True,\n",
        "        use_point_cloud=False,\n",
        "        use_attach=False,\n",
        "        wrt_world=True,\n",
        "        planner_name=\"RRTConnect\",\n",
        "        no_simplification=False,\n",
        "        constraint_function=None,\n",
        "        constraint_jacobian=None,\n",
        "        constraint_tolerance=1e-3,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        plan from a start configuration to a goal pose of the end-effector\n",
        "\n",
        "        Args:\n",
        "            goal_pose: [x,y,z,qw,qx,qy,qz] pose of the goal\n",
        "            current_qpos: current joint configuration (either full or move_group joints)\n",
        "            mask: if the value at a given index is True, the joint is *not* used in the\n",
        "                IK\n",
        "            time_step: time step for TOPPRA (time parameterization of path)\n",
        "            rrt_range: step size for RRT\n",
        "            planning_time: time limit for RRT\n",
        "            fix_joint_limits: if True, will clip the joint configuration to be within\n",
        "                the joint limits\n",
        "            use_point_cloud: if True, will use the point cloud to avoid collision\n",
        "            use_attach: if True, will consider the attached tool collision when planning\n",
        "            wrt_world: if true, interpret the target pose with respect to\n",
        "                the world frame instead of the base frame\n",
        "            verbose: if True, will print the log of OMPL and TOPPRA\n",
        "        \"\"\"\n",
        "        if mask is None:\n",
        "            mask = []\n",
        "        n = current_qpos.shape[0]\n",
        "        if fix_joint_limits:\n",
        "            for i in range(n):\n",
        "                if current_qpos[i] < self.joint_limits[i][0]:\n",
        "                    current_qpos[i] = self.joint_limits[i][0] + 1e-3\n",
        "                if current_qpos[i] > self.joint_limits[i][1]:\n",
        "                    current_qpos[i] = self.joint_limits[i][1] - 1e-3\n",
        "\n",
        "        if wrt_world:\n",
        "            goal_pose = self.transform_goal_to_wrt_base(goal_pose)\n",
        "\n",
        "        # we need to take only the move_group joints when planning\n",
        "        # idx = self.move_group_joint_indices\n",
        "\n",
        "        ik_status, goal_qpos = self.IK(goal_pose, current_qpos, mask)\n",
        "        if ik_status != \"Success\":\n",
        "            return {\"status\": ik_status}\n",
        "\n",
        "        if verbose:\n",
        "            print(\"IK results:\")\n",
        "            for i in range(len(goal_qpos)):\n",
        "                print(goal_qpos[i])\n",
        "\n",
        "        # goal_qpos_ = [goal_qpos[i][idx] for i in range(len(goal_qpos))]\n",
        "        self.robot.set_qpos(current_qpos, True)\n",
        "\n",
        "        ik_status, goal_qpos = self.IK(goal_pose, current_qpos, mask)\n",
        "        if ik_status != \"Success\":\n",
        "            return {\"status\": ik_status}\n",
        "\n",
        "        if verbose:\n",
        "            print(\"IK results:\")\n",
        "            for i in range(len(goal_qpos)):\n",
        "                print(goal_qpos[i])\n",
        "\n",
        "        return self.plan_qpos_to_qpos(\n",
        "            goal_qpos,\n",
        "            current_qpos,\n",
        "            time_step,\n",
        "            rrt_range,\n",
        "            planning_time,\n",
        "            fix_joint_limits,\n",
        "            use_point_cloud,\n",
        "            use_attach,\n",
        "            planner_name,\n",
        "            no_simplification,\n",
        "            constraint_function,\n",
        "            constraint_jacobian,\n",
        "            constraint_tolerance,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "\n",
        "    # plan_screw ankor\n",
        "    def plan_screw(\n",
        "        self,\n",
        "        target_pose,\n",
        "        qpos,\n",
        "        qpos_step=0.1,\n",
        "        time_step=0.1,\n",
        "        use_point_cloud=False,\n",
        "        use_attach=False,\n",
        "        wrt_world=True,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        # plan_screw ankor end\n",
        "        \"\"\"\n",
        "        Plan from a start configuration to a goal pose of the end-effector using\n",
        "        screw motion\n",
        "\n",
        "        Args:\n",
        "            target_pose: [x, y, z, qw, qx, qy, qz] pose of the goal\n",
        "            qpos: current joint configuration (either full or move_group joints)\n",
        "            qpos_step: size of the random step for RRT\n",
        "            time_step: time step for the discretization\n",
        "            use_point_cloud: if True, will use the point cloud to avoid collision\n",
        "            use_attach: if True, will use the attached tool to avoid collision\n",
        "            wrt_world: if True, interpret the target pose with respect to the\n",
        "                world frame instead of the base frame\n",
        "            verbose: if True, will print the log of TOPPRA\n",
        "        \"\"\"\n",
        "        self.planning_world.set_use_point_cloud(use_point_cloud)\n",
        "        self.planning_world.set_use_attach(use_attach)\n",
        "        qpos = self.pad_qpos(qpos.copy())\n",
        "        self.robot.set_qpos(qpos, True)\n",
        "        # 放在这里\n",
        "        print(\"Joint limits:\", self.joint_limits)\n",
        "        print(\"Joint limit size:\", len(self.joint_limits))\n",
        "        print(\"Qpos size:\", len(qpos))\n",
        "\n",
        "\n",
        "        if wrt_world:\n",
        "            target_pose = self.transform_goal_to_wrt_base(target_pose)\n",
        "\n",
        "        def pose7D2mat(pose):\n",
        "            mat = np.eye(4)\n",
        "            mat[0:3, 3] = pose[:3]\n",
        "            mat[0:3, 0:3] = quat2mat(pose[3:])\n",
        "            return mat\n",
        "\n",
        "        def skew(vec):\n",
        "            return np.array([\n",
        "                [0, -vec[2], vec[1]],\n",
        "                [vec[2], 0, -vec[0]],\n",
        "                [-vec[1], vec[0], 0],\n",
        "            ])\n",
        "\n",
        "        def pose2exp_coordinate(pose: np.ndarray) -> tuple[np.ndarray, float]:\n",
        "            def rot2so3(rotation: np.ndarray):\n",
        "                assert rotation.shape == (3, 3)\n",
        "                if np.isclose(rotation.trace(), 3):\n",
        "                    return np.zeros(3), 1\n",
        "                if np.isclose(rotation.trace(), -1):\n",
        "                    return np.zeros(3), -1e6\n",
        "                theta = np.arccos((rotation.trace() - 1) / 2)\n",
        "                omega = (\n",
        "                    1\n",
        "                    / 2\n",
        "                    / np.sin(theta)\n",
        "                    * np.array([\n",
        "                        rotation[2, 1] - rotation[1, 2],\n",
        "                        rotation[0, 2] - rotation[2, 0],\n",
        "                        rotation[1, 0] - rotation[0, 1],\n",
        "                    ]).T\n",
        "                )\n",
        "                return omega, theta\n",
        "\n",
        "            omega, theta = rot2so3(pose[:3, :3])\n",
        "            if theta < -1e5:\n",
        "                return omega, theta\n",
        "            ss = skew(omega)\n",
        "            inv_left_jacobian = (\n",
        "                np.eye(3) / theta\n",
        "                - 0.5 * ss\n",
        "                + (1.0 / theta - 0.5 / np.tan(theta / 2)) * ss @ ss\n",
        "            )\n",
        "            v = inv_left_jacobian @ pose[:3, 3]\n",
        "            return np.concatenate([v, omega]), theta\n",
        "\n",
        "        self.pinocchio_model.compute_forward_kinematics(qpos)\n",
        "        ee_index = self.link_name_2_idx[self.move_group]\n",
        "        current_p = pose7D2mat(self.pinocchio_model.get_link_pose(ee_index))\n",
        "        target_p = pose7D2mat(target_pose)\n",
        "        relative_transform = target_p @ np.linalg.inv(current_p)\n",
        "\n",
        "        omega, theta = pose2exp_coordinate(relative_transform)\n",
        "\n",
        "        if theta < -1e4:\n",
        "            return {\"status\": \"screw plan failed.\"}\n",
        "        omega = omega.reshape((-1, 1)) * theta\n",
        "\n",
        "        index = self.move_group_joint_indices\n",
        "        path = [np.copy(qpos[index])]\n",
        "\n",
        "\n",
        "        print(\"Relative theta =\", theta)\n",
        "        print(\"Initial omega =\", omega)\n",
        "        print(\"Start qpos =\", qpos)\n",
        "        print(f\"Start planning. Initial theta = {theta:.4f}\")\n",
        "        print(f\"Initial qpos: {qpos}\")\n",
        "        collisions = self.planning_world.collide_full()\n",
        "        if collisions:\n",
        "            print(\"🔍 Initial configuration already in collision!\")\n",
        "            for c in collisions:\n",
        "                print(f\"🔸 {c.link_name1} ⛔ {c.link_name2}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        while True:\n",
        "            self.pinocchio_model.compute_full_jacobian(qpos)\n",
        "            J = self.pinocchio_model.get_link_jacobian(ee_index, local=False)\n",
        "            delta_q = np.linalg.pinv(J) @ omega\n",
        "            delta_q *= qpos_step / (np.linalg.norm(delta_q))\n",
        "            delta_twist = J @ delta_q\n",
        "\n",
        "            flag = False\n",
        "            if np.linalg.norm(delta_twist) > np.linalg.norm(omega):\n",
        "                ratio = np.linalg.norm(omega) / np.linalg.norm(delta_twist)\n",
        "                delta_q = delta_q * ratio\n",
        "                delta_twist = delta_twist * ratio\n",
        "                flag = True\n",
        "\n",
        "            qpos += delta_q.reshape(-1)\n",
        "            omega -= delta_twist\n",
        "\n",
        "            def check_joint_limit(q):\n",
        "                n = len(q)\n",
        "                for i in range(n):\n",
        "                    if q[i] < self.joint_limits[i][0] - 1e-3:\n",
        "                        print(f\"❗ Joint {i} too low: {q[i]:.3f} < {self.joint_limits[i][0]:.3f}\")\n",
        "                        return False\n",
        "                    if q[i] > self.joint_limits[i][1] + 1e-3:\n",
        "                        print(f\"❗ Joint {i} too high: {q[i]:.3f} > {self.joint_limits[i][1]:.3f}\")\n",
        "                        return False\n",
        "                return True\n",
        "\n",
        "            # def check_joint_limit(q):\n",
        "            #     n = len(q)\n",
        "            #     for i in range(n):\n",
        "            #         if (\n",
        "            #             q[i] < self.joint_limits[i][0] - 1e-3\n",
        "            #             or q[i] > self.joint_limits[i][1] + 1e-3\n",
        "            #         ):\n",
        "            #             return False\n",
        "            #     return True\n",
        "\n",
        "            within_joint_limit = check_joint_limit(qpos)\n",
        "            self.planning_world.set_qpos_all(qpos[index])\n",
        "            collide = self.planning_world.collide()\n",
        "\n",
        "            if collide:\n",
        "                print(\"❌ Collision detected!\")\n",
        "            if not within_joint_limit:\n",
        "                print(\"❌ Joint limit exceeded!\")\n",
        "            if np.linalg.norm(delta_twist) < 1e-4:\n",
        "                print(\"✅ Reached convergence, but early exit.\")\n",
        "\n",
        "            if np.linalg.norm(delta_twist) < 1e-4 or collide or not within_joint_limit:\n",
        "                return {\"status\": \"screw plan failed\"}\n",
        "\n",
        "            path.append(np.copy(qpos[index]))\n",
        "\n",
        "            if flag:\n",
        "                if verbose:\n",
        "                    ta.setup_logging(\"INFO\")\n",
        "                else:\n",
        "                    ta.setup_logging(\"WARNING\")\n",
        "                times, pos, vel, acc, duration = self.TOPP(np.vstack(path), time_step)\n",
        "                return {\n",
        "                    \"status\": \"Success\",\n",
        "                    \"time\": times,\n",
        "                    \"position\": pos,\n",
        "                    \"velocity\": vel,\n",
        "                    \"acceleration\": acc,\n",
        "                    \"duration\": duration,\n",
        "                }"
      ],
      "metadata": {
        "id": "qFRF4O8Puqbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}